{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sandaruwanWGMA/Text-Sequence-Prediction-Shakespearean-Sonnets/blob/main/Text_Sequence_Prediction_Shakespearean_Sonnets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFWbEb6uGbN-"
      },
      "source": [
        "## Text Sequence Prediction: Shakespearean Sonnets\n",
        "\n",
        "In this phase of the project, the focus shifts to predicting the subsequent word in a text sequence. Leveraging the principles covered thus far, I am tasked with crafting a model tailored for this predictive task. The training dataset for this endeavor is sourced from the captivating sonnets of William Shakespeare.\n",
        "\n",
        "To kickstart this endeavor, I am in the process of implementing the predictive model, drawing insights from the techniques introduced during the course of the week. The model's architecture will be designed to effectively anticipate the next word in a given sequence, showcasing the convergence of theory and practical application.\n",
        "\n",
        "In tandem with model creation, the development of pivotal helper functions is underway. These functions are crafted to pre-process the dataset, ensuring it aligns seamlessly with the model's requirements. Data organization and formatting are key aspects addressed by these helper functions, laying a solid foundation for a successful training phase.\n",
        "\n",
        "As I immerse myself in this segment of the project, the integration of predictive modeling, meticulous training procedures, and thoughtful data pre-processing takes center stage. The ultimate goal is to fine-tune the model to predict the next word with precision, creating a captivating fusion of language and machine learning within the realm of Shakespearean sonnets.\n",
        "\n",
        "*   List item\n",
        "*   List item\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BOwsuGQQY9OL",
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTxqlHqKHzhr"
      },
      "source": [
        "For this assignment you will be using the [Shakespeare Sonnets Dataset](https://www.opensourceshakespeare.org/views/sonnets/sonnet_view.php?range=viewrange&sonnetrange1=1&sonnetrange2=154), which contains more than 2000 lines of text extracted from Shakespeare's sonnets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WZ4qOUzujMP6",
        "tags": [
          "graded"
        ],
        "outputId": "0780082a-a325-4649-d9a3-26af623dbde5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:138: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=108jAePKK4R3BVYBbYJZ32JWUwxeMg20K\n",
            "To: /content/sonnets.txt\n",
            "100% 93.6k/93.6k [00:00<00:00, 111MB/s]\n"
          ]
        }
      ],
      "source": [
        "# sonnets.txt\n",
        "!gdown --id 108jAePKK4R3BVYBbYJZ32JWUwxeMg20K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Pfd-nYKij5yY",
        "tags": [
          "graded"
        ],
        "outputId": "d4973da3-4b9b-4f43-ff4e-ac0528e8f29c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 2159 lines of sonnets\n",
            "\n",
            "The first 5 lines look like this:\n",
            "\n",
            "from fairest creatures we desire increase,\n",
            "that thereby beauty's rose might never die,\n",
            "but as the riper should by time decease,\n",
            "his tender heir might bear his memory:\n",
            "but thou, contracted to thine own bright eyes,\n"
          ]
        }
      ],
      "source": [
        "# Define path for file with sonnets\n",
        "SONNETS_FILE = './sonnets.txt'\n",
        "\n",
        "# Read the data\n",
        "with open('./sonnets.txt') as f:\n",
        "    data = f.read()\n",
        "\n",
        "# Convert to lower case and save as a list\n",
        "corpus = data.lower().split(\"\\n\")\n",
        "\n",
        "print(f\"There are {len(corpus)} lines of sonnets\\n\")\n",
        "print(f\"The first 5 lines look like this:\\n\")\n",
        "for i in range(5):\n",
        "  print(corpus[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imB15zrSNhA1"
      },
      "source": [
        "## Tokenizing the text\n",
        "\n",
        "Fit the Tokenizer to the corpus and save the total number of words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AAhM_qAZk0o5",
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "total_words = len(tokenizer.word_index) + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77-0sA46OETa"
      },
      "source": [
        "When converting the text into sequences you can use the `texts_to_sequences` method as you have done throughout this course.\n",
        "\n",
        "In the next graded function you will need to process this corpus one line at a time. Given this, it is important to keep in mind that the way you are feeding the data unto this method affects the result. Check the following example to make this clearer.\n",
        "\n",
        "The first example of the corpus is a string and looks like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tqhPxdeXlfjh",
        "tags": [
          "graded"
        ],
        "outputId": "e042d9bc-17cf-4ef3-a23b-5724448b30a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'from fairest creatures we desire increase,'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "corpus[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFMP4z11O3os"
      },
      "source": [
        "If we pass this text directly into the `texts_to_sequences` method, an unexpected result will occur. The reason lies in the fact that `texts_to_sequences` expects a list of texts as input, but when provided with a single string, it interprets each character as an individual text unit. Consequently, the method returns a sequence of word indices for each character rather than for the entire sentence.\n",
        "\n",
        "To address this, it's crucial to wrap the text in a list before passing it to `texts_to_sequences`. This ensures that the method interprets the entire string as a single text sequence, facilitating the correct conversion to a sequence of word indices. This small adjustment is essential to obtaining the intended result when processing text through the tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "EMSEhmbzNZCE",
        "tags": [
          "graded"
        ],
        "outputId": "ddbc309f-d49c-4459-c009-38eb340eb2d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[],\n",
              " [],\n",
              " [58],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [17],\n",
              " [6],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [17],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [6],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [6],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [17],\n",
              " [],\n",
              " [],\n",
              " []]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "tokenizer.texts_to_sequences(corpus[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPZmZtpEPEeI"
      },
      "source": [
        "The error occurred because the `texts_to_sequences` method expects a list, and in my initial implementation, I inadvertently provided a string. While a string is iterable in Python, it resulted in obtaining the word index for each individual character rather than for the entire sequence.\n",
        "\n",
        "To rectify this, it's crucial to encapsulate the example within a list before passing it to the `texts_to_sequences` method. This ensures that the method interprets the input as a single sequence rather than a sequence of characters. The corrected approach involves wrapping the example in square brackets to explicitly denote it as a list, allowing for accurate processing and conversion to sequences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Qmgo-vXhk4nd",
        "tags": [
          "graded"
        ],
        "outputId": "7468dda6-0cff-4108-99b2-6c31bf526bf0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[34, 417, 877, 166, 213, 517]]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "tokenizer.texts_to_sequences([corpus[0]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DU7wK-eQ5dc"
      },
      "source": [
        "Notice that you received the sequence wrapped inside a list so in order to get only the desired sequence you need to explicitly get the first item in the list like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "kpTy8WmIQ57P",
        "tags": [
          "graded"
        ],
        "outputId": "ebbf270f-1007-4f57-ec6d-b8f446ea395f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[34, 417, 877, 166, 213, 517]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "tokenizer.texts_to_sequences([corpus[0]])[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oqy9KjXRJ9A"
      },
      "source": [
        "## Generating n_grams\n",
        "\n",
        "In the ongoing task of refining our language model, I've completed the `n_gram_seqs` function, a critical component in the pre-processing pipeline. This function takes a fitted tokenizer and a corpus, represented as a list of strings, as inputs. Its primary objective is to generate n-gram sequences for each line within the corpus.\n",
        "\n",
        "To achieve this, the function iterates through each line, converting it into a sequence of word indices using the previously fitted tokenizer. Subsequently, it constructs n-gram sequences by sliding a window of size `n` through the sequence. These n-gram sequences are then appended to a list, forming the basis of our model's training data.\n",
        "\n",
        "By encapsulating the logic for n-gram sequence generation within this function, we streamline the pre-processing phase, ensuring that our model receives input data in a format conducive to effective learning. This meticulous approach sets the stage for training a language model adept at predicting the next word in Shakespeare's sonnets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "iy4baJMDl6kj",
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: n_gram_seqs\n",
        "def n_gram_seqs(corpus, tokenizer):\n",
        "    \"\"\"\n",
        "    Generates a list of n-gram sequences\n",
        "\n",
        "    Args:\n",
        "        corpus (list of string): lines of texts to generate n-grams for\n",
        "        tokenizer (object): an instance of the Tokenizer class containing the word-index dictionary\n",
        "\n",
        "    Returns:\n",
        "        input_sequences (list of int): the n-gram sequences for each line in the corpus\n",
        "    \"\"\"\n",
        "    input_sequences = []\n",
        "\n",
        "    ### START CODE HERE\n",
        "    for line in corpus:\n",
        "      token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "\n",
        "      for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "    ### END CODE HERE\n",
        "\n",
        "    return input_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "DlKqW2pfM7G3",
        "tags": [
          "graded"
        ],
        "outputId": "369e0ff6-3400-40fb-c008-741c1aeecc66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_gram sequences for first example look like this:\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[34, 417],\n",
              " [34, 417, 877],\n",
              " [34, 417, 877, 166],\n",
              " [34, 417, 877, 166, 213],\n",
              " [34, 417, 877, 166, 213, 517]]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# Test your function with one example\n",
        "first_example_sequence = n_gram_seqs([corpus[0]], tokenizer)\n",
        "\n",
        "print(\"n_gram sequences for first example look like this:\\n\")\n",
        "first_example_sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HL8Ug6UU0Jt"
      },
      "source": [
        "**Expected Output:**\n",
        "\n",
        "```\n",
        "n_gram sequences for first example look like this:\n",
        "\n",
        "[[34, 417],\n",
        " [34, 417, 877],\n",
        " [34, 417, 877, 166],\n",
        " [34, 417, 877, 166, 213],\n",
        " [34, 417, 877, 166, 213, 517]]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "wtPpCcBjNc4c",
        "tags": [
          "graded"
        ],
        "outputId": "2c7b49c8-240b-4110-a9a1-83367800f3c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_gram sequences for next 3 examples look like this:\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[8, 878],\n",
              " [8, 878, 134],\n",
              " [8, 878, 134, 351],\n",
              " [8, 878, 134, 351, 102],\n",
              " [8, 878, 134, 351, 102, 156],\n",
              " [8, 878, 134, 351, 102, 156, 199],\n",
              " [16, 22],\n",
              " [16, 22, 2],\n",
              " [16, 22, 2, 879],\n",
              " [16, 22, 2, 879, 61],\n",
              " [16, 22, 2, 879, 61, 30],\n",
              " [16, 22, 2, 879, 61, 30, 48],\n",
              " [16, 22, 2, 879, 61, 30, 48, 634],\n",
              " [25, 311],\n",
              " [25, 311, 635],\n",
              " [25, 311, 635, 102],\n",
              " [25, 311, 635, 102, 200],\n",
              " [25, 311, 635, 102, 200, 25],\n",
              " [25, 311, 635, 102, 200, 25, 278]]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# Test your function with a bigger corpus\n",
        "next_3_examples_sequence = n_gram_seqs(corpus[1:4], tokenizer)\n",
        "\n",
        "print(\"n_gram sequences for next 3 examples look like this:\\n\")\n",
        "next_3_examples_sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIzecMczU9UB"
      },
      "source": [
        "**Expected Output:**\n",
        "\n",
        "```\n",
        "n_gram sequences for next 3 examples look like this:\n",
        "\n",
        "[[8, 878],\n",
        " [8, 878, 134],\n",
        " [8, 878, 134, 351],\n",
        " [8, 878, 134, 351, 102],\n",
        " [8, 878, 134, 351, 102, 156],\n",
        " [8, 878, 134, 351, 102, 156, 199],\n",
        " [16, 22],\n",
        " [16, 22, 2],\n",
        " [16, 22, 2, 879],\n",
        " [16, 22, 2, 879, 61],\n",
        " [16, 22, 2, 879, 61, 30],\n",
        " [16, 22, 2, 879, 61, 30, 48],\n",
        " [16, 22, 2, 879, 61, 30, 48, 634],\n",
        " [25, 311],\n",
        " [25, 311, 635],\n",
        " [25, 311, 635, 102],\n",
        " [25, 311, 635, 102, 200],\n",
        " [25, 311, 635, 102, 200, 25],\n",
        " [25, 311, 635, 102, 200, 25, 278]]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dx3V_RjFWQSu"
      },
      "source": [
        "Apply the `n_gram_seqs` transformation to the whole corpus and save the maximum sequence length to use it later:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "laMwiRUpmuSd",
        "tags": [
          "graded"
        ],
        "outputId": "68e65fac-0a0f-44a1-8f27-5d8f9142354b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_grams of input_sequences have length: 15462\n",
            "maximum length of sequences is: 11\n"
          ]
        }
      ],
      "source": [
        "# Apply the n_gram_seqs transformation to the whole corpus\n",
        "input_sequences = n_gram_seqs(corpus, tokenizer)\n",
        "\n",
        "# Save max length\n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "\n",
        "print(f\"n_grams of input_sequences have length: {len(input_sequences)}\")\n",
        "print(f\"maximum length of sequences is: {max_sequence_len}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHY7HroqWq12"
      },
      "source": [
        "## Add padding to the sequences\n",
        "\n",
        "Now code the `pad_seqs` function which will pad any given sequences to the desired maximum length. Notice that this function receives a list of sequences and should return a numpy array with the padded sequences:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "cellView": "code",
        "id": "WW1-qAZaWOhC",
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: pad_seqs\n",
        "def pad_seqs(input_sequences, maxlen):\n",
        "    \"\"\"\n",
        "    Pads tokenized sequences to the same length\n",
        "\n",
        "    Args:\n",
        "        input_sequences (list of int): tokenized sequences to pad\n",
        "        maxlen (int): maximum length of the token sequences\n",
        "\n",
        "    Returns:\n",
        "        padded_sequences (array of int): tokenized sequences padded to the same length\n",
        "    \"\"\"\n",
        "    ### START CODE HERE\n",
        "    padded_sequences = pad_sequences(input_sequences, maxlen=maxlen, padding='pre')\n",
        "\n",
        "    return padded_sequences\n",
        "    ### END CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "IqVQ0pb3YHLr",
        "tags": [
          "graded"
        ],
        "outputId": "9e4da0df-24fd-4676-cfe5-7a5cd1e4e5d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0,  34, 417],\n",
              "       [  0,   0,  34, 417, 877],\n",
              "       [  0,  34, 417, 877, 166],\n",
              "       [ 34, 417, 877, 166, 213],\n",
              "       [417, 877, 166, 213, 517]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# Test your function with the n_grams_seq of the first example\n",
        "first_padded_seq = pad_seqs(first_example_sequence, len(first_example_sequence))\n",
        "first_padded_seq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Re_avDznXRnU"
      },
      "source": [
        "**Expected Output:**\n",
        "\n",
        "```\n",
        "array([[  0,   0,   0,  34, 417],\n",
        "       [  0,   0,  34, 417, 877],\n",
        "       [  0,  34, 417, 877, 166],\n",
        "       [ 34, 417, 877, 166, 213],\n",
        "       [417, 877, 166, 213, 517]], dtype=int32)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "j56_UCOBYzZt",
        "tags": [
          "graded"
        ],
        "outputId": "e202767c-088b-43d7-8b01-538098967efe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0,   0,   0,   0,   8, 878],\n",
              "       [  0,   0,   0,   0,   0,   8, 878, 134],\n",
              "       [  0,   0,   0,   0,   8, 878, 134, 351],\n",
              "       [  0,   0,   0,   8, 878, 134, 351, 102],\n",
              "       [  0,   0,   8, 878, 134, 351, 102, 156],\n",
              "       [  0,   8, 878, 134, 351, 102, 156, 199],\n",
              "       [  0,   0,   0,   0,   0,   0,  16,  22],\n",
              "       [  0,   0,   0,   0,   0,  16,  22,   2],\n",
              "       [  0,   0,   0,   0,  16,  22,   2, 879],\n",
              "       [  0,   0,   0,  16,  22,   2, 879,  61],\n",
              "       [  0,   0,  16,  22,   2, 879,  61,  30],\n",
              "       [  0,  16,  22,   2, 879,  61,  30,  48],\n",
              "       [ 16,  22,   2, 879,  61,  30,  48, 634],\n",
              "       [  0,   0,   0,   0,   0,   0,  25, 311],\n",
              "       [  0,   0,   0,   0,   0,  25, 311, 635],\n",
              "       [  0,   0,   0,   0,  25, 311, 635, 102],\n",
              "       [  0,   0,   0,  25, 311, 635, 102, 200],\n",
              "       [  0,   0,  25, 311, 635, 102, 200,  25],\n",
              "       [  0,  25, 311, 635, 102, 200,  25, 278]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# Test your function with the n_grams_seq of the next 3 examples\n",
        "next_3_padded_seq = pad_seqs(next_3_examples_sequence, max([len(s) for s in next_3_examples_sequence]))\n",
        "next_3_padded_seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "rgK-Q_micEYA",
        "tags": [
          "graded"
        ],
        "outputId": "d64ba636-88a3-4751-f85c-fc8e5648821e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "padded corpus has shape: (15462, 11)\n"
          ]
        }
      ],
      "source": [
        "# Pad the whole corpus\n",
        "input_sequences = pad_seqs(input_sequences, max_sequence_len)\n",
        "\n",
        "print(f\"padded corpus has shape: {input_sequences.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59RD1YYNc7CW"
      },
      "source": [
        "**Expected Output:**\n",
        "\n",
        "```\n",
        "padded corpus has shape: (15462, 11)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbOidyPrXxf7"
      },
      "source": [
        "## Split the data into features and labels\n",
        "\n",
        "As I progress in my project, a crucial step before feeding the data into the neural network involves splitting it into features and labels. To accomplish this, I've implemented the `features_and_labels` function, which plays a pivotal role in shaping the input data for effective model training.\n",
        "\n",
        "This function takes the padded n-gram sequences as input and is designed to return a tuple containing the features and the corresponding one-hot encoded labels. Notably, the features consist of the padded n-gram sequences with the last word removed, while the labels represent the removed word.\n",
        "\n",
        "An additional parameter in this function is the total number of words in the corpus. This parameter proves vital during the one-hot encoding process, ensuring that every word in the corpus is appropriately represented in the categorical labels. For a refresher on the `to_categorical` function, I referred to the [docs](https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical), ensuring precise implementation.\n",
        "\n",
        "With the `features_and_labels` function in place, the data is poised for seamless integration into the neural network, aligning with the overarching goal of training a language model adept at predicting the next word in Shakespeare's sonnets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "cellView": "code",
        "id": "9WGGbYdnZdmJ",
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: features_and_labels\n",
        "def features_and_labels(input_sequences, total_words):\n",
        "    \"\"\"\n",
        "    Generates features and labels from n-grams\n",
        "\n",
        "    Args:\n",
        "        input_sequences (list of int): sequences to split features and labels from\n",
        "        total_words (int): vocabulary size\n",
        "\n",
        "    Returns:\n",
        "        features, one_hot_labels (array of int, array of int): arrays of features and one-hot encoded labels\n",
        "    \"\"\"\n",
        "    ### START CODE HERE\n",
        "    features = input_sequences[:,:-1]\n",
        "    labels = input_sequences[:,-1]\n",
        "    one_hot_labels = to_categorical(labels, num_classes=total_words)\n",
        "    ### END CODE HERE\n",
        "\n",
        "    return features, one_hot_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "23DolaBRaIAZ",
        "tags": [
          "graded"
        ],
        "outputId": "4f97f315-d9da-4494-f74d-cf83ed06ff75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "labels have shape: (5, 3211)\n",
            "\n",
            "features look like this:\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0,  34],\n",
              "       [  0,   0,  34, 417],\n",
              "       [  0,  34, 417, 877],\n",
              "       [ 34, 417, 877, 166],\n",
              "       [417, 877, 166, 213]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# Test your function with the padded n_grams_seq of the first example\n",
        "first_features, first_labels = features_and_labels(first_padded_seq, total_words)\n",
        "\n",
        "print(f\"labels have shape: {first_labels.shape}\")\n",
        "print(\"\\nfeatures look like this:\\n\")\n",
        "first_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "GRTuLEt3bRKa",
        "tags": [
          "graded"
        ],
        "outputId": "3c2744a7-557f-40a8-9758-e1e0b5349fbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "features have shape: (15462, 10)\n",
            "labels have shape: (15462, 3211)\n"
          ]
        }
      ],
      "source": [
        "# Split the whole corpus\n",
        "features, labels = features_and_labels(input_sequences, total_words)\n",
        "\n",
        "print(f\"features have shape: {features.shape}\")\n",
        "print(f\"labels have shape: {labels.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXSMK_HpdLns"
      },
      "source": [
        "**Expected Output:**\n",
        "\n",
        "```\n",
        "features have shape: (15462, 10)\n",
        "labels have shape: (15462, 3211)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltxaOCE_aU6J"
      },
      "source": [
        "## Create the model\n",
        "\n",
        "In crafting the neural network for text sequence prediction in Shakespeare's sonnets, I've opted for a concise architecture:\n",
        "\n",
        "- **Embedding Layer:** Initiated with an `output_dim` of 100 to capture word representations.\n",
        "  \n",
        "- **Bidirectional LSTM:** Leveraging the bidirectional nature of LSTM layers for enhanced sequential context.\n",
        "\n",
        "- **Output Layer:** Featuring the same units as total words, equipped with softmax activation for next-word prediction.\n",
        "\n",
        "This streamlined model, limited to two layers, balances complexity for optimal performance, targeting an accuracy of 80% or higher. The iterative refinement process remains pivotal in achieving accurate predictions within the sonnet context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "cellView": "code",
        "id": "XrE6kpJFfvRY",
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: create_model\n",
        "import tensorflow as tf\n",
        "def create_model(total_words, max_sequence_len):\n",
        "    \"\"\"\n",
        "    Creates a text generator model\n",
        "\n",
        "    Args:\n",
        "        total_words (int): size of the vocabulary for the Embedding layer input\n",
        "        max_sequence_len (int): length of the input sequences\n",
        "\n",
        "    Returns:\n",
        "        model (tf.keras Model): the text generator model\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "    ### START CODE HERE\n",
        "    model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
        "\n",
        "    model.add(Bidirectional(tf.keras.layers.GRU(64))),\n",
        "    model.add(Dense(total_words*6, activation='relu'))\n",
        "    model.add(Dense(total_words, activation='softmax'))\n",
        "\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    ### END CODE HERE\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "0IpX_Gu_gISk",
        "tags": [],
        "outputId": "d6aabb18-e7f0-4181-dbee-ef7547d3b8f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "484/484 [==============================] - 24s 40ms/step - loss: 6.7447 - accuracy: 0.0305\n",
            "Epoch 2/50\n",
            "484/484 [==============================] - 13s 28ms/step - loss: 6.1976 - accuracy: 0.0565\n",
            "Epoch 3/50\n",
            "484/484 [==============================] - 13s 27ms/step - loss: 5.6419 - accuracy: 0.0733\n",
            "Epoch 4/50\n",
            "484/484 [==============================] - 14s 28ms/step - loss: 5.0441 - accuracy: 0.1002\n",
            "Epoch 5/50\n",
            "484/484 [==============================] - 13s 27ms/step - loss: 4.3866 - accuracy: 0.1407\n",
            "Epoch 6/50\n",
            "484/484 [==============================] - 14s 29ms/step - loss: 3.6399 - accuracy: 0.2121\n",
            "Epoch 7/50\n",
            "484/484 [==============================] - 13s 27ms/step - loss: 2.8364 - accuracy: 0.3398\n",
            "Epoch 8/50\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 2.1225 - accuracy: 0.4822\n",
            "Epoch 9/50\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 1.5760 - accuracy: 0.6065\n",
            "Epoch 10/50\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 1.2152 - accuracy: 0.6939\n",
            "Epoch 11/50\n",
            "484/484 [==============================] - 13s 27ms/step - loss: 0.9700 - accuracy: 0.7557\n",
            "Epoch 12/50\n",
            "484/484 [==============================] - 12s 26ms/step - loss: 0.8249 - accuracy: 0.7942\n",
            "Epoch 13/50\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 0.7450 - accuracy: 0.8130\n",
            "Epoch 14/50\n",
            "484/484 [==============================] - 13s 27ms/step - loss: 0.6899 - accuracy: 0.8263\n",
            "Epoch 15/50\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 0.6750 - accuracy: 0.8278\n",
            "Epoch 16/50\n",
            "484/484 [==============================] - 13s 27ms/step - loss: 0.6482 - accuracy: 0.8345\n",
            "Epoch 17/50\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 0.6392 - accuracy: 0.8347\n",
            "Epoch 18/50\n",
            "484/484 [==============================] - 13s 28ms/step - loss: 0.6441 - accuracy: 0.8311\n",
            "Epoch 19/50\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 0.6226 - accuracy: 0.8354\n",
            "Epoch 20/50\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 0.6021 - accuracy: 0.8388\n",
            "Epoch 21/50\n",
            "484/484 [==============================] - 12s 25ms/step - loss: 0.5842 - accuracy: 0.8419\n",
            "Epoch 22/50\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 0.5879 - accuracy: 0.8419\n",
            "Epoch 23/50\n",
            "484/484 [==============================] - 12s 26ms/step - loss: 0.5812 - accuracy: 0.8420\n",
            "Epoch 24/50\n",
            "484/484 [==============================] - 12s 25ms/step - loss: 0.5740 - accuracy: 0.8421\n",
            "Epoch 25/50\n",
            "484/484 [==============================] - 12s 26ms/step - loss: 0.5631 - accuracy: 0.8452\n",
            "Epoch 26/50\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 0.5564 - accuracy: 0.8434\n",
            "Epoch 27/50\n",
            "484/484 [==============================] - 13s 27ms/step - loss: 0.5543 - accuracy: 0.8428\n",
            "Epoch 28/50\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 0.5630 - accuracy: 0.8396\n",
            "Epoch 29/50\n",
            "484/484 [==============================] - 12s 26ms/step - loss: 0.5701 - accuracy: 0.8399\n",
            "Epoch 30/50\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 0.5482 - accuracy: 0.8451\n",
            "Epoch 31/50\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 0.5260 - accuracy: 0.8471\n",
            "Epoch 32/50\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 0.5184 - accuracy: 0.8498\n",
            "Epoch 33/50\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 0.5235 - accuracy: 0.8472\n",
            "Epoch 34/50\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 0.5319 - accuracy: 0.8469\n",
            "Epoch 35/50\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 0.5360 - accuracy: 0.8461\n",
            "Epoch 36/50\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 0.5628 - accuracy: 0.8375\n",
            "Epoch 37/50\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 0.5340 - accuracy: 0.8435\n",
            "Epoch 38/50\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 0.5173 - accuracy: 0.8472\n",
            "Epoch 39/50\n",
            "484/484 [==============================] - 13s 27ms/step - loss: 0.5163 - accuracy: 0.8474\n",
            "Epoch 40/50\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 0.5054 - accuracy: 0.8479\n",
            "Epoch 41/50\n",
            "484/484 [==============================] - 12s 26ms/step - loss: 0.5043 - accuracy: 0.8494\n",
            "Epoch 42/50\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 0.5050 - accuracy: 0.8499\n",
            "Epoch 43/50\n",
            "484/484 [==============================] - 13s 27ms/step - loss: 0.5065 - accuracy: 0.8498\n",
            "Epoch 44/50\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 0.5291 - accuracy: 0.8436\n",
            "Epoch 45/50\n",
            "484/484 [==============================] - 12s 26ms/step - loss: 0.5320 - accuracy: 0.8424\n",
            "Epoch 46/50\n",
            "484/484 [==============================] - 12s 25ms/step - loss: 0.5097 - accuracy: 0.8456\n",
            "Epoch 47/50\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 0.5028 - accuracy: 0.8483\n",
            "Epoch 48/50\n",
            "484/484 [==============================] - 12s 25ms/step - loss: 0.5012 - accuracy: 0.8489\n",
            "Epoch 49/50\n",
            "484/484 [==============================] - 12s 25ms/step - loss: 0.4905 - accuracy: 0.8510\n",
            "Epoch 50/50\n",
            "484/484 [==============================] - 12s 26ms/step - loss: 0.4895 - accuracy: 0.8507\n"
          ]
        }
      ],
      "source": [
        "# Get the untrained model\n",
        "model = create_model(total_words, max_sequence_len)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(features, labels, epochs=50, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gy72RPgly55q"
      },
      "source": [
        "**To pass this assignment, your model should achieve a training accuracy of at least 80%**. If your model didn't achieve this threshold, try training again with a different model architecture, consider increasing the number of unit in your `LSTM` layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "1fXTEO3GJ282",
        "tags": [],
        "outputId": "91390c8f-5c15-4530-a968-5ed68c015d02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 887
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGzCAYAAAD9pBdvAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA5FElEQVR4nO3de1yUZf7/8fdwGlTkoCCioSiapqaWB5ZOtolRW5ZttXby1MHW1U5ubbqVlu2GHbYt07T8ZVarm2VZWWa5pLaVZaHmITNBE1YFNRMUFJC5f3/c3xkYQWU43XN4PR+P+zH33Nz3zIdbZN5c13Vft80wDEMAAAAWCbK6AAAAENgIIwAAwFKEEQAAYCnCCAAAsBRhBAAAWIowAgAALEUYAQAAliKMAAAASxFGAACApQgjgJ8YPXq0kpKS6nTso48+KpvN1rAFAUAtEUaARmaz2Wq1rFq1yupSAcASNu5NAzSuf/3rX27PX3/9da1YsUJvvPGG2/YhQ4YoPj6+zu9TXl4uh8Mhu93u8bHHjx/X8ePHFR4eXuf3B4C6IowATWzChAmaNWuWTvdfr6SkRM2bN2+iqlAbhmHo2LFjatasmdWlAH6FbhrAC1x88cXq1auXsrKydNFFF6l58+b661//Kkl6//33dcUVV6hdu3ay2+1KTk7W448/roqKCrfXOHHMyM8//yybzaZnnnlGL7/8spKTk2W32zVgwAB9++23bsfWNGbEZrNpwoQJeu+999SrVy/Z7Xb17NlTy5cvr1b/qlWr1L9/f4WHhys5OVkvvfRSrceh/Pe//9X111+vDh06yG63KzExUffdd5+OHj1abd8ff/xRf/jDHxQXF6dmzZqpW7dueuihh9z22b17t2677TbX+erUqZPGjRunsrKyk36vkjR//nzZbDb9/PPPrm1JSUm68sor9cknn6h///5q1qyZXnrpJUnSq6++qksuuURt2rSR3W5Xjx49NHv27Bq/x48//liDBg1Sy5YtFRkZqQEDBmjhwoWSpKlTpyo0NFT79++vdtzYsWMVHR2tY8eOnfY8Ar4sxOoCAJh++eUXXX755brhhht0yy23uLps5s+fr4iICE2cOFERERH67LPPNGXKFBUVFenpp58+7esuXLhQhw8f1p133imbzaannnpKv//977Vjxw6Fhoae8tgvvvhC7777rv70pz+pZcuWmjFjhq699lrl5uaqdevWkqT169frsssuU0JCgh577DFVVFRo2rRpiouLq9X3/fbbb6ukpETjxo1T69attXbtWr3wwgv63//+p7ffftu138aNG3XhhRcqNDRUY8eOVVJSknJycrR06VL9/e9/lyTt2bNHAwcO1KFDhzR27Fh1795du3fv1uLFi1VSUqKwsLBa1VTVtm3bdOONN+rOO+/UHXfcoW7dukmSZs+erZ49e+qqq65SSEiIli5dqj/96U9yOBwaP3686/j58+fr1ltvVc+ePTV58mRFR0dr/fr1Wr58uW666SaNGDFC06ZN06JFizRhwgTXcWVlZVq8eLGuvfZaus/g/wwATWr8+PHGif/1Bg0aZEgy5syZU23/kpKSatvuvPNOo3nz5saxY8dc20aNGmV07NjR9Xznzp2GJKN169bGwYMHXdvff/99Q5KxdOlS17apU6dWq0mSERYWZmRnZ7u2ff/994Yk44UXXnBtGzp0qNG8eXNj9+7drm3bt283QkJCqr1mTWr6/jIyMgybzWbs2rXLte2iiy4yWrZs6bbNMAzD4XC41keOHGkEBQUZ3377bbXXdO5X0/dqGIbx6quvGpKMnTt3urZ17NjRkGQsX768VnWnp6cbnTt3dj0/dOiQ0bJlSyMlJcU4evToSetOTU01UlJS3L7+7rvvGpKMlStXVnsfwN/QTQN4CbvdrjFjxlTbXnV8wuHDh3XgwAFdeOGFKikp0Y8//nja1x0+fLhiYmJczy+88EJJ0o4dO057bFpampKTk13Pe/furcjISNexFRUV+s9//qNhw4apXbt2rv26dOmiyy+//LSvL7l/f8XFxTpw4IDOO+88GYah9evXS5L279+vzz//XLfeeqs6dOjgdryzy8XhcOi9997T0KFD1b9//2rvU9dLlzt16qT09PRT1l1YWKgDBw5o0KBB2rFjhwoLCyVJK1as0OHDhzVp0qRqrRtV6xk5cqS++eYb5eTkuLYtWLBAiYmJGjRoUJ3qBnwJYQTwEu3bt6+xG2HLli265pprFBUVpcjISMXFxemWW26RJNeH3qmc+OHtDCa//vqrx8c6j3ceu2/fPh09elRdunSptl9N22qSm5ur0aNHq1WrVoqIiFBcXJzrA9j5/TnDT69evU76Ovv371dRUdEp96mLTp061bj9yy+/VFpamlq0aKHo6GjFxcW5xvk463aGi9PVNHz4cNntdi1YsMB1/Icffqibb76Z+V8QEBgzAniJmq7QOHTokAYNGqTIyEhNmzZNycnJCg8P17p16/Tggw/K4XCc9nWDg4Nr3G7U4kK6+hxbGxUVFRoyZIgOHjyoBx98UN27d1eLFi20e/dujR49ulbfn6dO9uF+4oBgp5r+XXJycjR48GB1795dzz77rBITExUWFqZly5bpn//8p8d1x8TE6Morr9SCBQs0ZcoULV68WKWlpa7QCfg7wgjgxVatWqVffvlF7777ri666CLX9p07d1pYVaU2bdooPDxc2dnZ1b5W07YTbdq0ST/99JNee+01jRw50rV9xYoVbvt17txZkrR58+aTvlZcXJwiIyNPuY9U2TJ06NAhRUdHu7bv2rXrtPU6LV26VKWlpfrggw/cWo9Wrlzptp+zi2vz5s2nbSkaOXKkrr76an377bdasGCBzjnnHPXs2bPWNQG+jG4awIs5WyaqtkSUlZXpxRdftKokN8HBwUpLS9N7772nPXv2uLZnZ2fr448/rtXxkvv3ZxiGnn/+ebf94uLidNFFF2nevHnKzc11+5rz2KCgIA0bNkxLly7Vd999V+29nPs5A8Lnn3/u+lpxcbFee+2109Z7qroLCwv16quvuu136aWXqmXLlsrIyKh2ee6JrUuXX365YmNj9eSTT2r16tW0iiCg0DICeLHzzjtPMTExGjVqlO6++27ZbDa98cYbDdZN0hAeffRRffrppzr//PM1btw4VVRUaObMmerVq5c2bNhwymO7d++u5ORk3X///dq9e7ciIyP1zjvv1DieZcaMGbrgggt07rnnauzYserUqZN+/vlnffTRR673eeKJJ/Tpp59q0KBBGjt2rM466yzt3btXb7/9tr744gtFR0fr0ksvVYcOHXTbbbfpgQceUHBwsObNm6e4uLhqQedkLr30UoWFhWno0KG68847deTIEc2dO1dt2rTR3r17XftFRkbqn//8p26//XYNGDBAN910k2JiYvT999+rpKTELQCFhobqhhtu0MyZMxUcHKwbb7yxVrUA/oCWEcCLtW7dWh9++KESEhL08MMP65lnntGQIUP01FNPWV2aS79+/fTxxx8rJiZGjzzyiF555RVNmzZNgwcPPu38GKGhoVq6dKn69u2rjIwMPfbYY+ratatef/31avv26dNHX3/9tS666CLNnj1bd999t9555x1dddVVrn3at2+vb775Rtddd50WLFigu+++W6+//rouvvhi12y2oaGhWrJkiZKTk/XII49oxowZuv32293m+Didbt26afHixbLZbLr//vs1Z84cjR07Vvfcc0+1fW+77TZ98MEHioyM1OOPP64HH3xQ69atq/FqI2dX1eDBg5WQkFDregBfx3TwABrFsGHDtGXLFm3fvt3qUnzG999/r759++r111/XiBEjrC4HaDK0jACotxOnbt++fbuWLVumiy++2JqCfNTcuXMVERGh3//+91aXAjQpxowAqLfOnTtr9OjR6ty5s3bt2qXZs2crLCxMf/nLX6wuzScsXbpUP/zwg15++WVNmDBBLVq0sLokoEnRTQOg3saMGaOVK1cqPz9fdrtdqampeuKJJ3TuuedaXZpPSEpKUkFBgdLT0/XGG2+oZcuWVpcENCnCCAAAsBRjRgAAgKUIIwAAwFI+MYDV4XBoz549atmyJTeNAgDARxiGocOHD6tdu3YKCjp5+4dPhJE9e/YoMTHR6jIAAEAd5OXl6Ywzzjjp130ijDhHlufl5SkyMtLiagAAQG0UFRUpMTHxtFeI+UQYcXbNREZGEkYAAPAxpxtiwQBWAABgKcIIAACwFGEEAABYijACAAAsRRgBAACWIowAAABLEUYAAIClCCMAAMBShBEAAGApwggAALAUYQQAAFiKMAIAACzlEzfKAwB/YBhSYaEUGSkF8adggzMMqbxcKiszF+e6JDnv02azVS7O50FBUmysb/+bGIZ06JCUny8VFLg/HjsmhYRIwcHmY9X1qtuuv15KSLCmfsIIADSQ8nJp925p166al9xcqbRUCg2VEhOlDh3MpWPHyvUOHcyvtWhh9XfjuYoK88Pf4TDXHY7qS0WFdPy4ea6cYcG5XnVbSYn066/mcuhQ5XrVpajIPJ9VX6euYmOlIUOk9HTzsV27BjsttWIYZnhw/qzs3m2eg9JSM0xUfXSuHzsm/fJLZfBwBq+6GjiQMALAzzkc0p490pEj5i/+mBjzrzJfcPiwlJ1t/sKvadm3z3zcv9/8Pk+nvFzascNcTiU4+NRLWJjUvLnUrJn5WHXd+ZiYKN1wgxl4GkpZmbR9u7Rli/TDD+bjli3mtuPHG+59GkJYWOW6YVQuJz4/cED697/NRZLOPtsMJunp0gUXSOHh9avDMMyfk+xsKSfn5EG1vqKjpfh4qW1bc4mPN38OnCHw+HH39arPY2Pr//51ZTMM5z+L9yoqKlJUVJQKCwsVGRlpdTkATuLYMWnnTvOXbU6O+WHrXN+50/2Xrc0mtWpl/gKMjZVat65cj4kxP0ztdnMJD3d/tNvNX7AdOpjHOZvcG0purvTll+byxRfSpk21CxmSWVvV1o6OHd2Xtm3N0JKb6744P5B27TLDT0Oy2aTBg6UxY6RrrjHPbW2VlkpffSV9/rm0eXP9QkdQkLmEhJitQ84lLMz9eWioWWNMzKmXyEjzZyIsrHJxvl5YmBnYavOzUV4uff219Mkn5pKVVRlYJLOWQYOkc8813/NUS1mZ+fOenW2ep+zsyuXIkVPXYbNJ7dtXtpBFRLj/zFdddy6tW1eGjvj4+oemhlbbz2/CCOAHfv218i8u5y/CnBxp717z6yf2kZ/43DDMD9u6PDrXHQ5zPMSpBAeb3Q9FRQ33vUdHS126VF+6dpXi4k7+YWQY5l+E5eXS1q2V4ePLL6X//a/6/vHxZhN2mzaVv/hrWtq0qf/Yg6Ii6ehRs75TLc7ujKNH3R+rrn/5pfTZZ5WvHRUl3XijGUwGDKh+fhwOacMGKTNT+s9/pP/+13ytE7VsKfXsKfXoYT4612NiKkNHUJD5bx4U1PCBsTEdOGB+7598In36qdmi1xBsNjNkJCdLSUnVg+oZZ5hhyp8QRgA/c/iwtG1b5bJ9e2Xw+PVXq6urFBlp/rLt3Ln6Y4cO5l/F5eXSwYPmL/2qyy+/mI8HD7r3jdfUX37kiNlXfioREeZftTU1T5+qpSM4WDrnHOn88yuXph5D0JB27pRee02aP99seXHq0cMMJWlp0tq15gfwZ5+Z/w5VtW0rXXKJ1K9fZfg44wzfChh1ZRhma9Cnn0o//2wGRedy+LD78yNHzOCVlGSG4RMDcqdOZmtGICGMAD7IMMwPix9+cA8e27ZVtnKcTNu25i+85OTKxzPOMH851tRHXvW58y/Xkz0610+23bneunXjdJucTEmJ2RVUtSnc2Tyel+fe1H4qUVFSampl8Bg40DcHkJ6OwyGtWiXNmye9844Z6mrSsqV08cVm105amhlAAiF41FdFhfkzF8JoTBfCCFBPRUXSihXSRx9J69dXNo1XvSqg6lUCQUHmXz7du0tnnVX5eLK/IMvLpR9/NF97/XqzaXzDBvPKgZNp00bq1s1czjyz8i+uzp3988OzPo4dM4Pd8ePVL2c88dLGiAjfvqyzLgoLpUWLzGDy/fdmAHOGjwED/K+7ANYgjAB1kJMjffihuaxeXb9LBZ1atDCDSffuZnDYvdsMH5s31zx6PjS0MnCcuERH178eAGgqtf38pjEJAcswzGb+776rDCA//ui+T9eu0pVXmk3WLVrUPDDPuZSXm10EW7ear7N1q/m8uNgcnZ+VVb2Gli2lvn3N8QnnnGOu9+jhfjkiAPg7wgj8ksMhLVlitj78+qs5INL5WHX9xJaPkBDpoovMAHLFFWZXiCdSU92fl5ebrS1Vw0nbtpXho1OnwOseAIATEUbgd9aule66y3ysjdhY6Xe/MwPIpZeagxkbSmhoZRfNsGEN97oA4E8II/Ab+/ZJkyebA/IkswvkD38w55po1cqc/6BVK/f1mBiz+4UrBQDAOoQR+LzycunFF6WpUysn3Ro5UnrySbNLBADg3Qgj8GkrV5pdMlu2mM/PPVd64QXpvPOsrQsAUHsMnYNPys01u2AuucQMIq1bSy+9ZI4TIYgAgG+hZQQ+Z9Ei6dZbzctyg4KkceOkadPMMSAAAN9Dywh8hmFIjz5q3g69pMS8rfe6ddLMmQQRAPBltIzAJxw9Ko0eLb31lvn8/vul6dPNiccAAL6tTi0js2bNUlJSksLDw5WSkqK1p5nQ4bnnnlO3bt3UrFkzJSYm6r777tOxk92hCTjB3r3SoEFmEAkNlV55RXr6aYIIAPgLj1tGFi1apIkTJ2rOnDlKSUnRc889p/T0dG3btk1t2rSptv/ChQs1adIkzZs3T+edd55++uknjR49WjabTc8++2yDfBPwX+vXS0OHmvdzadVKevddM5gAAPyHxzfKS0lJ0YABAzRz5kxJksPhUGJiou666y5NmjSp2v4TJkzQ1q1blZmZ6dr25z//Wd98842++OKLGt+jtLRUpVXuIFZUVKTExERulBdg3n1XGjHCHB/Svbt575jkZKurAgDUVm1vlOdRN01ZWZmysrKUlpZW+QJBQUpLS9OaNWtqPOa8885TVlaWqytnx44dWrZsmX73u9+d9H0yMjIUFRXlWhITEz0pEz7OMKSMDOnaa80gkp4uff01QQQA/JVHYeTAgQOqqKhQfHy82/b4+Hjl5+fXeMxNN92kadOm6YILLlBoaKiSk5N18cUX669//etJ32fy5MkqLCx0LXl5eZ6UCR9WWiqNGiU5fzzuustsEWnI+8UAALxLo1/au2rVKj3xxBN68cUXtW7dOr377rv66KOP9Pjjj5/0GLvdrsjISLcFgeH++6U33jAHp774ojRjhnknXQCA//Lo13xsbKyCg4NVUFDgtr2goEBtT3ITkEceeUQjRozQ7bffLkk6++yzVVxcrLFjx+qhhx5SEPdPx//ZuNEMIJI5XuSqq6ytBwDQNDxKAmFhYerXr5/bYFSHw6HMzEylpqbWeExJSUm1wBH8f9dkejh2Fn7MMKR77pEcDum66wgiABBIPG4AnzhxokaNGqX+/ftr4MCBeu6551RcXKwxY8ZIkkaOHKn27dsrIyNDkjR06FA9++yzOuecc5SSkqLs7Gw98sgjGjp0qCuUAO+8I61aJYWHS888Y3U1AICm5HEYGT58uPbv368pU6YoPz9fffv21fLly12DWnNzc91aQh5++GHZbDY9/PDD2r17t+Li4jR06FD9/e9/b7jvAj6tpET685/N9QcflDp2tLYeAEDT8nieESvU9jpl+KbHHjPvOZOYKP34o9S8udUVAQAaQqPMMwI0tNxc6cknzfVnniGIAEAgIozAUg88YN4Eb9Ag6frrra4GAGAFwggss2qVefO7oCDp+eclm83qigAAViCMwBLHj5uX8krSnXdKffpYWw8AwDqEEVhi7lxzkrOYGOkUk/ECAAIAYQRN7uBB6eGHzfVp06TWra2tBwBgLcIImtyUKWYg6dVL+uMfra4GAGA1wgia1KZN0uzZ5vrzz3MTPAAAYQRNqOr9Z669VrrkEqsrAgB4A8IImsy770orV3L/GQCAO8IImozzqpkHHpCSkiwtBQDgRQgjaBIbN0rffy+FhUn33mt1NQAAb0IYQZN44w3z8YorpFatrK0FAOBdCCNodBUV0sKF5vrIkdbWAgDwPoQRNLrPPpP27DFbRH73O6urAQB4G8IIGp2zi2b4cHPMCAAAVRFG0KiOHDEv6ZWkESOsrQUA4J0II2hUS5ZIxcVSly7Sb35jdTUAAG9EGEGjcnbR3HKLZLNZWwsAwDsRRtBo9uyRMjPNdbpoAAAnQxhBo1m40LwPzfnnS507W10NAMBbEUbQaJxdNLSKAABOhTCCRrFxo7mEhUl/+IPV1QAAvBlhBI3C2Spy5ZVSTIy1tQAAvBthBA2uokJasMBcp4sGAHA6hBE0uMxMae9epn8HANQOYQQNztlFc8MNTP8OADg9wggaFNO/AwA8RRhBg1qyRCopkbp2lVJSrK4GAOALCCNoUEz/DgDwFGEEDabq9O+33GJtLQAA30EYQYNxTv9+wQVM/w4AqD3CCBrM66+bjwxcBQB4gjCCBvH999KmTealvNdfb3U1AABfQhhBg3AOXB06lOnfAQCeIYyg3gxD+ve/zXW6aAAAniKMoN5++MG8kqZZM+myy6yuBgDgawgjqLfPPjMfL7hAstutrQUA4HsII6g3Zxi55BJr6wAA+CbCCOqlokJatcpcJ4wAAOqCMIJ6+f576dAhKTJSOvdcq6sBAPgiwgjqxdlFM2iQFBJibS0AAN9EGEG9OMPIb39rbR0AAN9FGEGdlZdLn39urjNeBABQV4QR1Nm330rFxVLr1tLZZ1tdDQDAVxFGUGdVu2iC+EkCANQRHyGos5UrzUe6aAAA9UEYQZ0cOyZ9+aW5ThgBANQHYQR1smaNVFoqJSRIZ55pdTUAAF9GGEGdVJ0C3mazthYAgG8jjKBOuB8NAKChEEbgsSNHpLVrzXXCCACgvggj8NgXX0jHj0udOklJSVZXAwDwdYQReIwuGgBAQyKMwGPcjwYA0JAII/DIr79K69aZ64QRAEBDIIzAI6tXS4Yhde8utWtndTUAAH9AGIFHmAIeANDQCCPwCINXAQANjTCCWisokDZvNtcvvtjSUgAAfoQwglpbtcp87NNHat3a0lIAAH6EMIJao4sGANAYCCOoNcIIAKAxEEZQK3l5Una2FBwsXXSR1dUAAPwJYQS14rykt39/KTLS2loAAP6FMIJaoYsGANBYCCM4LcPgfjQAgMZDGMFp5eSYY0ZCQ6Xzz7e6GgCAvyGM4LScrSKpqVLz5tbWAgDwP4QRnBb3owEANKY6hZFZs2YpKSlJ4eHhSklJ0dq1a0+5/6FDhzR+/HglJCTIbrfrzDPP1LJly+pUMJpW1fEihBEAQGMI8fSARYsWaeLEiZozZ45SUlL03HPPKT09Xdu2bVObNm2q7V9WVqYhQ4aoTZs2Wrx4sdq3b69du3YpOjq6IepHI/vpJ2nfPqlZM2ngQKurAQD4I4/DyLPPPqs77rhDY8aMkSTNmTNHH330kebNm6dJkyZV23/evHk6ePCgvvrqK4WGhkqSkpKS6lc1msyGDeZjnz6S3W5pKQAAP+VRN01ZWZmysrKUlpZW+QJBQUpLS9OaNWtqPOaDDz5Qamqqxo8fr/j4ePXq1UtPPPGEKioqTvo+paWlKioqcltgjY0bzcfeva2tAwDgvzwKIwcOHFBFRYXi4+PdtsfHxys/P7/GY3bs2KHFixeroqJCy5Yt0yOPPKJ//OMf+tvf/nbS98nIyFBUVJRrSUxM9KRMNCBnGOnTx9o6AAD+q9GvpnE4HGrTpo1efvll9evXT8OHD9dDDz2kOXPmnPSYyZMnq7Cw0LXk5eU1dpk4ie+/Nx9pGQEANBaPxozExsYqODhYBQUFbtsLCgrUtm3bGo9JSEhQaGiogoODXdvOOuss5efnq6ysTGFhYdWOsdvtsjNAwXK//mpOdiZJZ59tbS0AAP/lUctIWFiY+vXrp8zMTNc2h8OhzMxMpaam1njM+eefr+zsbDkcDte2n376SQkJCTUGEXiPTZvMx44dpagoa2sBAPgvj7tpJk6cqLlz5+q1117T1q1bNW7cOBUXF7uurhk5cqQmT57s2n/cuHE6ePCg7rnnHv3000/66KOP9MQTT2j8+PEN912gUTi7aBgvAgBoTB5f2jt8+HDt379fU6ZMUX5+vvr27avly5e7BrXm5uYqKKgy4yQmJuqTTz7Rfffdp969e6t9+/a655579OCDDzbcd4FGwZU0AICmYDMMw7C6iNMpKipSVFSUCgsLFRkZaXU5ASMlRVq7VnrrLen6662uBgDga2r7+c29aVCjigpp82ZznZYRAEBjIoygRjk5UkmJOQ18ly5WVwMA8GeEEdTIOV6kVy+pylXZAAA0OMIIasTgVQBAUyGMoEaEEQBAUyGMoEbMMQIAaCqEEVRTWCj9/LO5zjTwAIDGRhhBNc5Les84Q2rVytpaAAD+jzCCarhTLwCgKRFGUI1z8CrjRQAATYEwgmq4kgYA0JQII3DjcEibNpnrhBEAQFMgjMDNzp3SkSOS3S6deabV1QAAAgFhBG6cXTQ9e0ohIdbWAgAIDIQRuGG8CACgqRFG4IYwAgBoaoQRuGEaeABAUyOMwOXIESknx1xnGngAQFMhjMDFOQ18QoIUF2dtLQCAwEEYgQvTwAMArEAYgQvTwAMArEAYgQtX0gAArEAYgSTJMAgjAABrEEYgSdq1SyoqkkJDpW7drK4GABBICCOQVNkq0qOHFBZmbS0AgMBCGIEkumgAANYhjEASYQQAYB3CCCQxDTwAwDqEEaikRNq+3VynZQQA0NQII9CWLealvW3aSPHxVlcDAAg0hBEwDTwAwFKEETANPADAUoQRcCUNAMBShJEAxzTwAACrEUYC3P/+J/36qxQSIp11ltXVAAACEWEkwDlbRbp3l+x2a2sBAAQmwkiAo4sGAGA1wkiAI4wAAKxGGAlwzDECALAaYSSAHTsmbdtmrjPHCADAKoSRAPbDD5LDIbVuLSUkWF0NACBQEUYC2KZN5uPZZ0s2m7W1AAACF2EkgG3daj727GltHQCAwEYYCWA//mg+du9ubR0AgMBGGAlgzpYRwggAwEqEkQBVVibl5JjrTAMPALASYSRAZWdLFRVSy5ZSu3ZWVwMACGSEkQBVdbwIV9IAAKxEGAlQDF4FAHgLwkiAYvAqAMBbEEYClLNlhMGrAACrEUYCkGHQTQMA8B6EkQC0e7d05IgUEiJ16WJ1NQCAQEcYCUDOVpHkZCk01NpaAAAgjAQgBq8CALwJYSQAMXgVAOBNCCMBiJYRAIA3IYwEIFpGAADehDASYAoLpb17zfVu3aytBQAAiTAScJytIgkJUlSUtbUAACARRgIOXTQAAG9DGAkwDF4FAHgbwkiAoWUEAOBtCCMBhnvSAAC8DWEkgJSVSdnZ5jphBADgLQgjASQnR6qokCIipPbtra4GAAATYSSAVB28arNZWwsAAE6EkQDC4FUAgDcijAQQBq8CALwRYSSAMMcIAMAbEUYChGHQTQMA8E51CiOzZs1SUlKSwsPDlZKSorVr19bquDfffFM2m03Dhg2ry9uiHvbskY4ckYKDpeRkq6sBAKCSx2Fk0aJFmjhxoqZOnap169apT58+Sk9P1759+0553M8//6z7779fF154YZ2LRd05u2i6dJHCwqytBQCAqjwOI88++6zuuOMOjRkzRj169NCcOXPUvHlzzZs376THVFRU6Oabb9Zjjz2mzp0716tg1A2DVwEA3sqjMFJWVqasrCylpaVVvkBQkNLS0rRmzZqTHjdt2jS1adNGt912W63ep7S0VEVFRW4L6ofBqwAAb+VRGDlw4IAqKioUHx/vtj0+Pl75+fk1HvPFF1/olVde0dy5c2v9PhkZGYqKinItiYmJnpSJGjB4FQDgrRr1aprDhw9rxIgRmjt3rmJjY2t93OTJk1VYWOha8vLyGrHKwEA3DQDAW4V4snNsbKyCg4NVUFDgtr2goEBt27attn9OTo5+/vlnDR061LXN4XCYbxwSom3btim5hks77Ha77Ha7J6XhFAoLzatpJMIIAMD7eNQyEhYWpn79+ikzM9O1zeFwKDMzU6mpqdX27969uzZt2qQNGza4lquuukq//e1vtWHDBrpfmsi2beZjQoIUFWVtLQAAnMijlhFJmjhxokaNGqX+/ftr4MCBeu6551RcXKwxY8ZIkkaOHKn27dsrIyND4eHh6tWrl9vx0dHRklRtOxoPg1cBAN7M4zAyfPhw7d+/X1OmTFF+fr769u2r5cuXuwa15ubmKiiIiV29CYNXAQDezGYYhmF1EadTVFSkqKgoFRYWKjIy0upyfM4110jvvSfNmCHddZfV1QAAAkVtP79pwggAzm4aWkYAAN6IMOLnysulnBxznTEjAABvRBjxc9nZ0vHjUkSE1L691dUAAFAdYcTPVZ3szGazthYAAGpCGPFzzLwKAPB2hBE/x+BVAIC3I4z4OVpGAADejjDixwyDMAIA8H6EET+2Z490+LAUHCx16WJ1NQAA1Iww4secrSLJyVJYmLW1AABwMoQRP8bgVQCALyCM+DHGiwAAfAFhxI85W0YIIwAAb0YY8WPOlhG6aQAA3oww4qeKisyraSRaRgAA3o0w4qecrSIJCVJUlLW1AABwKoQRP8XgVQCAryCM+CkGrwIAfAVhxE9t3mw+9uxpbR0AAJwOYcRPbdpkPp59trV1AABwOoQRP1RUJO3aZa4TRgAA3o4w4oecXTTt20sxMdbWAgDA6RBG/BBdNAAAX0IY8UOEEQCALyGM+CHCCADAlxBG/IxhSBs3muuEEQCALyCM+Jndu6VDh6TgYG6QBwDwDYQRP+PsojnzTMlut7YWAABqgzDiZxgvAgDwNYQRP0MYAQD4GsKIn3GGkd69ra0DAIDaIoz4kfLyyrv10jICAPAVhBE/sn27VFYmRURIHTtaXQ0AALVDGPEjzi6aXr2kIP5lAQA+go8sP8LgVQCALyKM+BHCCADAFxFG/AhhBADgiwgjfuLwYWnnTnOdMAIA8CWEET+xZYv5mJAgtW5tbS0AAHiCMOIn6KIBAPgqwoifIIwAAHwVYcRPEEYAAL6KMOIHDIMwAgDwXYQRP5CfL/3yiznr6llnWV0NAACeIYz4AWerSNeuUrNm1tYCAICnCCN+YONG85EuGgCALyKM+AHGiwAAfBlhxA8QRgAAvoww4uOOH5d++MFc793b2loAAKgLwoiPy86WSkulFi2kTp2srgYAAM8RRnycs4umZ0/z0l4AAHwNH18+jvEiAABfRxjxcYQRAICvI4z4OMIIAMDXEUZ8WHGxtGOHuU4YAQD4KsKID9uyxbxJXny8FBdndTUAANQNYcSH0UUDAPAHhBEfRhgBAPgDwogPI4wAAPwBYcSHEUYAAP6AMOKjCgqk/fslm03q0cPqagAAqDvCiI9ytop06SI1b25tLQAA1AdhxEfRRQMA8BeEER9FGAEA+AvCiI/auNF8JIwAAHwdYcQHVVSYs69KhBEAgO8jjPignBzp2DGpWTMpOdnqagAAqB/CiA9yjhfp2VMKDra2FgAA6osw4oMYvAoA8CeEER9EGAEA+BPCiI85flxatcpc79/f0lIAAGgQdQojs2bNUlJSksLDw5WSkqK1a9eedN+5c+fqwgsvVExMjGJiYpSWlnbK/XFq//2vdPCg1Lq1lJpqdTUAANSfx2Fk0aJFmjhxoqZOnap169apT58+Sk9P1759+2rcf9WqVbrxxhu1cuVKrVmzRomJibr00ku1e/fuehcfiN57z3y86iopJMTSUgAAaBA2wzAMTw5ISUnRgAEDNHPmTEmSw+FQYmKi7rrrLk2aNOm0x1dUVCgmJkYzZ87UyJEja/WeRUVFioqKUmFhoSIjIz0p168YhpSUJOXmSu+/bwYSAAC8VW0/vz1qGSkrK1NWVpbS0tIqXyAoSGlpaVqzZk2tXqOkpETl5eVq1arVSfcpLS1VUVGR2wJp/XoziDRvLg0ZYnU1AAA0DI/CyIEDB1RRUaH4+Hi37fHx8crPz6/Vazz44INq166dW6A5UUZGhqKiolxLYmKiJ2X6rSVLzMfLLjMnPAMAwB806dU006dP15tvvqklS5YoPDz8pPtNnjxZhYWFriUvL68Jq/RezvEi11xjaRkAADQoj4ZAxsbGKjg4WAUFBW7bCwoK1LZt21Me+8wzz2j69On6z3/+o969e59yX7vdLrvd7klpfi87W9q82Ry0esUVVlcDAEDD8ahlJCwsTP369VNmZqZrm8PhUGZmplJPcZ3pU089pccff1zLly9XfybHqBNnF83FF0sxMZaWAgBAg/L44tCJEydq1KhR6t+/vwYOHKjnnntOxcXFGjNmjCRp5MiRat++vTIyMiRJTz75pKZMmaKFCxcqKSnJNbYkIiJCERERDfit+DdnF82wYVZWAQBAw/M4jAwfPlz79+/XlClTlJ+fr759+2r58uWuQa25ubkKCqpscJk9e7bKysp03XXXub3O1KlT9eijj9av+gCRny85L1a6+mprawEAoKF5PM+IFQJ9npGXX5buvFMaMEBi8loAgK9olHlGYA3neBGuogEA+CPCiJcrKpKc44UZLwIA8EeEES+3bJlUXi516yaddZbV1QAA0PAII16Oq2gAAP6OMOLFSkvNlhGJ8SIAAP9FGPFin30mHT4sJSSYV9IAAOCPCCNezHkVzbBhUhD/UgAAP8VHnJeqqJDef99cZ7wIAMCfEUa81NdfS/v2SVFR5v1oAADwV4QRL+W8iuaKK6SwMEtLAQCgURFGvJBhMOsqACBwEEa80JYtUk6OZLdLl11mdTUAADQuwogXcraKDBkiRURYWwsAAI2NMOKFmHUVABBICCNeZtcuad06c16Rq66yuhoAABofYcTLOOcWOf98KS7O2loAAGgKhBEvw1U0AIBAQxjxIllZ0qpV5jrjRQAAgYIw4iUMQ7r/fnP95pulTp2srQcAgKZCGPESH35otorY7dLf/251NQAANB3CiBcoL5ceeMBcv/deqWNHS8sBAKBJEUa8wNy50rZtUmysNHmy1dUAANC0CCMWKyyUpk411x991LxLLwAAgYQwYrEnn5QOHJC6dZPGjrW6GgAAmh5hxEK5udI//2muP/WUFBpqbT0AAFiBMGKhhx6Sjh2TBg2Shg61uhoAAKxBGLFIVpb0r3+Z6888I9ls1tYDAIBVCCMWqDrB2S23SP37W1sPAABWIoxYYOlSJjgDAMCJMNLEysulv/zFXL/vPqlDB2vrAQDAaoSRJuac4CwujgnOAACQCCNN6sQJziIjLS0HAACvQBhpQtOnV05wdscdVlcDAIB3IIw0kY0bmeAMAICaEEaawL595qRmpaXSZZcxwRkAAFURRhpZaal0zTXm1O9du0oLFjDBGQAAVRFGGpFhmDe/++orKTranF+kVSurqwIAwLsQRhrR009Lr78uBQdLb71lDlwFAADuCCON5IMPpEmTzPXnn5eGDLG2HgAAvBVhpBFs3CjddJPZTTNunDR+vNUVAQDgvQgjDcx55UxxsTR4sNkqAgAATo4w0oBOvHLmrbeYTwQAgNMhjDQQrpwBAKBuCCMNhCtnAAComxCrC/B1Dof0j39w5QwAAHVFGKmHAwekUaOkZcvM53ffzZUzAAB4ijBSR//9r3TjjdLu3VJ4uDRjhnT77VZXBQCA72HMiIccDumJJ6Tf/tYMIt26Sd98I91xB/ecAQCgLmgZ8cC+fdItt0grVpjPR4yQXnxRioiwti4AAHwZYaSWVq0yZ1Xdu1dq1kyaNUsaPZrWEAAA6otumtOoqJCmTTNnU927V+rRQ/ruO2nMGIIIAAANgZaRUzAMc1Dq/Pnm81tvlV54QWre3NKyAADwK4SRU/jb38wgEhwsvfKKeRkvAABoWHTTnMS//iVNmWKuz5pFEAEAoLEQRmqwerXZJSNJDzwg3XmntfUAAODPCCMn2LbNvPNuebl03XXS9OlWVwQAgH8jjFSxf7/0u99Jv/4q/eY35o3vgjhDAAA0Kj5q/8/Ro9JVV0k7dkidO0sffGDOJwIAABoXYUTmFO8jRkhffy3FxJg3vouLs7oqAAACA2FE0qRJ0jvvSKGh0pIl5v1mAABA0wj4MDJnjvT00+b6q69KgwZZWw8AAIEmoMPIxx9L48eb69OmSTffbG09AAAEooANI8XF0siR5niR0aOlhx+2uiIAAAJTwIaRFi2k99+X/vAH6aWXuOkdAABWCeh705x3nrkAAADrBGzLCAAA8A6EEQAAYCnCCAAAsBRhBAAAWIowAgAALEUYAQAAlqpTGJk1a5aSkpIUHh6ulJQUrV279pT7v/322+revbvCw8N19tlna9myZXUqFgAA+B+Pw8iiRYs0ceJETZ06VevWrVOfPn2Unp6uffv21bj/V199pRtvvFG33Xab1q9fr2HDhmnYsGHavHlzvYsHAAC+z2YYhuHJASkpKRowYIBmzpwpSXI4HEpMTNRdd92lSZMmVdt/+PDhKi4u1ocffuja9pvf/EZ9+/bVnDlzavWeRUVFioqKUmFhoSIjIz0pFwAAWKS2n98etYyUlZUpKytLaWlplS8QFKS0tDStWbOmxmPWrFnjtr8kpaenn3R/SSotLVVRUZHbAgAA/JNHYeTAgQOqqKhQfHy82/b4+Hjl5+fXeEx+fr5H+0tSRkaGoqKiXEtiYqInZQIAAB/ilVfTTJ48WYWFha4lLy/P6pIAAEAj8ehGebGxsQoODlZBQYHb9oKCArVt27bGY9q2bevR/pJkt9tlt9s9KQ0AAPgoj8JIWFiY+vXrp8zMTA0bNkySOYA1MzNTEyZMqPGY1NRUZWZm6t5773VtW7FihVJTU2v9vs4xtowdAQDAdzg/t097rYzhoTfffNOw2+3G/PnzjR9++MEYO3asER0dbeTn5xuGYRgjRowwJk2a5Nr/yy+/NEJCQoxnnnnG2Lp1qzF16lQjNDTU2LRpU63fMy8vz5DEwsLCwsLC4oNLXl7eKT/nPWoZkcxLdffv368pU6YoPz9fffv21fLly12DVHNzcxUUVDkU5bzzztPChQv18MMP669//au6du2q9957T7169ar1e7Zr1055eXlq2bKlbDabpyWfVFFRkRITE5WXl8clw02A8920ON9Ni/PdtDjfTauu59swDB0+fFjt2rU75X4ezzPiT5i/pGlxvpsW57tpcb6bFue7aTX2+fbKq2kAAEDgIIwAAABLBXQYsdvtmjp1KpcRNxHOd9PifDctznfT4nw3rcY+3wE9ZgQAAFgvoFtGAACA9QgjAADAUoQRAABgKcIIAACwFGEEAABYKqDDyKxZs5SUlKTw8HClpKRo7dq1VpfkFz7//HMNHTpU7dq1k81m03vvvef2dcMwNGXKFCUkJKhZs2ZKS0vT9u3brSnWD2RkZGjAgAFq2bKl2rRpo2HDhmnbtm1u+xw7dkzjx49X69atFRERoWuvvbba3bRRO7Nnz1bv3r0VGRmpyMhIpaam6uOPP3Z9nXPdeKZPny6bzeZ241XOd8N69NFHZbPZ3Jbu3bu7vt5Y5ztgw8iiRYs0ceJETZ06VevWrVOfPn2Unp6uffv2WV2azysuLlafPn00a9asGr/+1FNPacaMGZozZ46++eYbtWjRQunp6Tp27FgTV+ofVq9erfHjx+vrr7/WihUrVF5erksvvVTFxcWufe677z4tXbpUb7/9tlavXq09e/bo97//vYVV+64zzjhD06dPV1ZWlr777jtdcskluvrqq7VlyxZJnOvG8u233+qll15S79693bZzvhtez549tXfvXtfyxRdfuL7WaOfb07v2+ouBAwca48ePdz2vqKgw2rVrZ2RkZFhYlf+RZCxZssT13OFwGG3btjWefvpp17ZDhw4Zdrvd+Pe//21Bhf5n3759hiRj9erVhmGY5zc0NNR4++23Xfts3brVkGSsWbPGqjL9SkxMjPH//t//41w3ksOHDxtdu3Y1VqxYYQwaNMi45557DMPgZ7sxTJ061ejTp0+NX2vM8x2QLSNlZWXKyspSWlqaa1tQUJDS0tK0Zs0aCyvzfzt37lR+fr7buY+KilJKSgrnvoEUFhZKklq1aiVJysrKUnl5uds57969uzp06MA5r6eKigq9+eabKi4uVmpqKue6kYwfP15XXHGF23mV+NluLNu3b1e7du3UuXNn3XzzzcrNzZXUuOc7pF5H+6gDBw6ooqJC8fHxbtvj4+P1448/WlRVYMjPz5ekGs+982uoO4fDoXvvvVfnn3++evXqJck852FhYYqOjnbbl3Ned5s2bVJqaqqOHTumiIgILVmyRD169NCGDRs41w3szTff1Lp16/Ttt99W+xo/2w0vJSVF8+fPV7du3bR371499thjuvDCC7V58+ZGPd8BGUYAfzV+/Hht3rzZrY8XDa9bt27asGGDCgsLtXjxYo0aNUqrV6+2uiy/k5eXp3vuuUcrVqxQeHi41eUEhMsvv9y13rt3b6WkpKhjx45666231KxZs0Z734DspomNjVVwcHC1EcAFBQVq27atRVUFBuf55dw3vAkTJujDDz/UypUrdcYZZ7i2t23bVmVlZTp06JDb/pzzugsLC1OXLl3Ur18/ZWRkqE+fPnr++ec51w0sKytL+/bt07nnnquQkBCFhIRo9erVmjFjhkJCQhQfH8/5bmTR0dE688wzlZ2d3ag/3wEZRsLCwtSvXz9lZma6tjkcDmVmZio1NdXCyvxfp06d1LZtW7dzX1RUpG+++YZzX0eGYWjChAlasmSJPvvsM3Xq1Mnt6/369VNoaKjbOd+2bZtyc3M55w3E4XCotLSUc93ABg8erE2bNmnDhg2upX///rr55ptd65zvxnXkyBHl5OQoISGhcX++6zX81Ye9+eabht1uN+bPn2/88MMPxtixY43o6GgjPz/f6tJ83uHDh43169cb69evNyQZzz77rLF+/Xpj165dhmEYxvTp043o6Gjj/fffNzZu3GhcffXVRqdOnYyjR49aXLlvGjdunBEVFWWsWrXK2Lt3r2spKSlx7fPHP/7R6NChg/HZZ58Z3333nZGammqkpqZaWLXvmjRpkrF69Wpj586dxsaNG41JkyYZNpvN+PTTTw3D4Fw3tqpX0xgG57uh/fnPfzZWrVpl7Ny50/jyyy+NtLQ0IzY21ti3b59hGI13vgM2jBiGYbzwwgtGhw4djLCwMGPgwIHG119/bXVJfmHlypWGpGrLqFGjDMMwL+995JFHjPj4eMNutxuDBw82tm3bZm3RPqymcy3JePXVV137HD161PjTn/5kxMTEGM2bNzeuueYaY+/evdYV7cNuvfVWo2PHjkZYWJgRFxdnDB482BVEDINz3dhODCOc74Y1fPhwIyEhwQgLCzPat29vDB8+3MjOznZ9vbHOt80wDKN+bSsAAAB1F5BjRgAAgPcgjAAAAEsRRgAAgKUIIwAAwFKEEQAAYCnCCAAAsBRhBAAAWIowAgAALEUYAQAAliKMAAAASxFGAACApf4/O9mwkwjohLUAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAAGzCAYAAABzfl4TAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA5RUlEQVR4nO3de3hU1b3/8c/kNklIMsEAIZEQ7nehCIgIKApyFQGttS0qtPWoNBZQ7E9tFdGeGqw+0nqpF/oUztEq2lZArKggEAREw00jAoKEBCSAIGQgkIRk9u+PfWZIICGZZCZ7Lu/X86xn1uy5fbM7dT7stfbaNsMwDAEAAPhAhNUFAACA0EGwAAAAPkOwAAAAPkOwAAAAPkOwAAAAPkOwAAAAPkOwAAAAPkOwAAAAPkOwAAAAPkOwAMLE1KlT1a5duwa9ds6cObLZbL4tqJ4aUzeApkewACxms9nq1dasWWN1qQBQJxvXCgGs9frrr1e7/7//+79asWKFXnvttWrbr7/+eqWmpjb4c86ePSuXyyW73e71aysqKlRRUaHY2NgGf35DTZ06VWvWrNG+ffua/LMBeC/K6gKAcHfbbbdVu79x40atWLHigu3nO336tOLj4+v9OdHR0Q2qT5KioqIUFcV/LgDUjaEQIAgMGzZMvXr10ubNm3X11VcrPj5ev/vd7yRJS5cu1bhx45Seni673a6OHTvqD3/4gyorK6u9x/lzFfbt2yebzaZnnnlGr776qjp27Ci73a4BAwYoNze32mtrmmNhs9l07733asmSJerVq5fsdrt69uypDz744IL616xZo/79+ys2NlYdO3bUK6+80qh5GyUlJZo1a5YyMjJkt9vVtWtXPfPMMzr/AOyKFSs0ZMgQJScnKyEhQV27dvXsN7fnn39ePXv2VHx8vJo3b67+/fvrjTfeaFBdADhiAQSNY8eOacyYMfrpT3+q2267zTMssnDhQiUkJOj+++9XQkKCVq1apdmzZ8vpdOrpp5+u833feOMNnTx5UnfffbdsNpv+9Kc/6aabbtLevXvrPMqxbt06vfPOO/r1r3+txMREPffcc7r55ptVWFiolJQUSdLWrVs1evRopaWl6fHHH1dlZaWeeOIJtWzZskH7wTAM3XjjjVq9erV+9atf6Uc/+pE+/PBD/fa3v9V3332nefPmSZK2b9+uG264Qb1799YTTzwhu92uPXv2aP369Z73mj9/vqZPn64f//jHmjFjhkpLS/Xll1/qs88+089//vMG1QeEPQNAQMnKyjLO/7/mNddcY0gyXn755Quef/r06Qu23X333UZ8fLxRWlrq2TZlyhQjMzPTcz8/P9+QZKSkpBg//PCDZ/vSpUsNScayZcs82x577LELapJkxMTEGHv27PFs++KLLwxJxvPPP+/ZNn78eCM+Pt747rvvPNt2795tREVFXfCeNTm/7iVLlhiSjP/+7/+u9rwf//jHhs1m89Qzb948Q5Lx/fff1/reEyZMMHr27FlnDQDqj6EQIEjY7Xb94he/uGB7XFycp3/y5EkdPXpUQ4cO1enTp7Vz58463/fWW29V8+bNPfeHDh0qSdq7d2+drx0xYoQ6duzoud+7d28lJSV5XltZWamVK1dq4sSJSk9P9zyvU6dOGjNmTJ3vX5P3339fkZGRmj59erXts2bNkmEYWr58uSQpOTlZkjlU5HK5anyv5ORkHThw4IKhHwANR7AAgsSll16qmJiYC7Zv375dkyZNksPhUFJSklq2bOmZ+FlcXFzn+7Zt27bafXfIOH78uNevdb/e/dojR47ozJkz6tSp0wXPq2lbfRQUFCg9PV2JiYnVtnfv3t3zuGQGpsGDB+vOO+9UamqqfvrTn+rtt9+uFjIefPBBJSQk6IorrlDnzp2VlZVVbagEgPcIFkCQqHpkwu3EiRO65ppr9MUXX+iJJ57QsmXLtGLFCj311FOSVOu/1KuKjIyscbtRjzPRG/Naf4uLi9PatWu1cuVK3X777fryyy9166236vrrr/dMbO3evbt27dqlRYsWaciQIfr3v/+tIUOG6LHHHrO4eiB4ESyAILZmzRodO3ZMCxcu1IwZM3TDDTdoxIgR1YY2rNSqVSvFxsZqz549FzxW07b6yMzM1MGDB3Xy5Mlq293DPpmZmZ5tERERGj58uJ599ll9/fXX+uMf/6hVq1Zp9erVnuc0a9ZMt956qxYsWKDCwkKNGzdOf/zjH1VaWtqg+oBwR7AAgpj7iEHVIwTl5eX661//alVJ1URGRmrEiBFasmSJDh486Nm+Z88ez1wIb40dO1aVlZV64YUXqm2fN2+ebDabZ+7GDz/8cMFrf/SjH0mSysrKJJln2lQVExOjHj16yDAMnT17tkH1AeGO002BIHbVVVepefPmmjJliqZPny6bzabXXnstIIYi3ObMmaOPPvpIgwcP1rRp0zyhoFevXtq2bZvX7zd+/Hhde+21+v3vf699+/apT58++uijj7R06VLNnDnTM5n0iSee0Nq1azVu3DhlZmbqyJEj+utf/6o2bdpoyJAhkqSRI0eqdevWGjx4sFJTU7Vjxw698MILGjdu3AVzOADUD8ECCGIpKSl67733NGvWLD3yyCNq3ry5brvtNg0fPlyjRo2yujxJUr9+/bR8+XI98MADevTRR5WRkaEnnnhCO3bsqNdZK+eLiIjQu+++q9mzZ+utt97SggUL1K5dOz399NOaNWuW53k33nij9u3bp7///e86evSoWrRooWuuuUaPP/64HA6HJOnuu+/WP/7xDz377LM6deqU2rRpo+nTp+uRRx7x2d8PhBuuFQLAEhMnTtT27du1e/duq0sB4EPMsQDgd2fOnKl2f/fu3Xr//fc1bNgwawoC4DccsQDgd2lpaZo6dao6dOiggoICvfTSSyorK9PWrVvVuXNnq8sD4EPMsQDgd6NHj9abb76pQ4cOyW63a9CgQXryyScJFUAI4ogFAADwGeZYAAAAnyFYAAAAn/FqjkW7du08F/ip6te//rVefPHFer2Hy+XSwYMHlZiYKJvN5s3HAwAAixiGoZMnTyo9PV0REbUfl/AqWOTm5nou3iNJX331la6//nrdcsst9X6PgwcPKiMjw5uPBQAAAWL//v1q06ZNrY97FSxatmxZ7f7cuXPVsWNHXXPNNfV+D/cyufv371dSUpI3Hw8AACzidDqVkZFR53L3DT7dtLy8XK+//rruv//+iw5plJWVeS74I8lzRcKkpCSCBQAAQaauaQwNnry5ZMkSnThxQlOnTr3o87Kzs+VwODyNYRAAAEJXg9exGDVqlGJiYrRs2bKLPu/8IxbuQynFxcUcsQAAIEg4nU45HI46f78bNBRSUFCglStX6p133qnzuXa7XXa7vSEfAwAAgkyDgsWCBQvUqlUrjRs3ztf1AAD8wDAMVVRUVDuzD6gqMjJSUVFRjV4Kwutg4XK5tGDBAk2ZMkVRUVxqBAACXXl5uYqKinT69GmrS0GAi4+PV1pammJiYhr8Hl4ng5UrV6qwsFC//OUvG/yhAICm4XK5lJ+fr8jISKWnpysmJobFCXEBwzBUXl6u77//Xvn5+ercufNFF8G6GK+DxciRI8V1ywAgOJSXl8vlcikjI0Px8fFWl4MAFhcXp+joaBUUFKi8vFyxsbENeh+uFQIAYaCh//pEePHF94RvGgAA8BmCBQAA8BmCBQAgLLRr105//vOf6/38NWvWyGaz6cSJE36rKRQRLAAAAcVms120zZkzp0Hvm5ubq7vuuqvez7/qqqtUVFQkh8PRoM+rr1ALMCGxEMWZM9KCBdLHH0tvvy1FRlpdEQCgoYqKijz9t956S7Nnz9auXbs82xISEjx9wzBUWVlZr3WVzr9Cd11iYmLUunVrr16DEDliERUlPfKI9M470oYNVlcDAIHNMKSSkqZv9V2poHXr1p7mcDhks9k893fu3KnExEQtX75c/fr1k91u17p16/Ttt99qwoQJSk1NVUJCggYMGKCVK1dWe9/zh0JsNpv+9re/adKkSYqPj1fnzp317rvveh4//0jCwoULlZycrA8//FDdu3dXQkKCRo8eXS0IVVRUaPr06UpOTlZKSooefPBBTZkyRRMnTmzo/1w6fvy47rjjDjVv3lzx8fEaM2aMdu/e7Xm8oKBA48ePV/PmzdWsWTP17NlT77//vue1kydPVsuWLRUXF6fOnTtrwYIFDa6lPkIiWERHSzfcYPaXLLG0FAAIeKdPSwkJTd98ufDnQw89pLlz52rHjh3q3bu3Tp06pbFjx+rjjz/W1q1bNXr0aI0fP16FhYUXfZ/HH39cP/nJT/Tll19q7Nixmjx5sn744YeL7LvTeuaZZ/Taa69p7dq1Kiws1AMPPOB5/KmnntI//vEPLViwQOvXr5fT6dSSRv4wTZ06VZs2bdK7776rTz/9VIZhaOzYsTp79qwkKSsrS2VlZVq7dq3y8vL01FNPeY7qPProo/r666+1fPly7dixQy+99JJatGjRqHrqZDSx4uJiQ5JRXFzs0/f9978NQzKMDh0Mw+Xy6VsDQNA6c+aM8fXXXxtnzpzxbDt1yvzvZVO3U6e8r3/BggWGw+Hw3F+9erUhyViyZEmdr+3Zs6fx/PPPe+5nZmYa8+bN89yXZDzyyCNV9sspQ5KxfPnyap91/PhxTy2SjD179nhe8+KLLxqpqame+6mpqcbTTz/tuV9RUWG0bdvWmDBhQq11nv85VX3zzTeGJGP9+vWebUePHjXi4uKMt99+2zAMw7jsssuMOXPm1Pje48ePN37xi1/U+tnnq+n74lbf3++QmGMhSaNGSbGx0t69Ul6e1Lu31RUBQGCKj5dOnbLmc32lf//+1e6fOnVKc+bM0X/+8x8VFRWpoqJCZ86cqfOIRe8qPxbNmjVTUlKSjhw5Uuvz4+Pj1bFjR8/9tLQ0z/OLi4t1+PBhXXHFFZ7HIyMj1a9fP7lcLq/+PrcdO3YoKipKAwcO9GxLSUlR165dtWPHDknS9OnTNW3aNH300UcaMWKEbr75Zs/fNW3aNN18883asmWLRo4cqYkTJ+qqq65qUC31FRJDIZLUrJk0cqTZZzgEAGpns5n/zWzq5stLlDRr1qza/QceeECLFy/Wk08+qU8++UTbtm3TZZddpvLy8ou+T3R09Hn7xnbREFDT8w2LL3Nx5513au/evbr99tuVl5en/v376/nnn5ckjRkzRgUFBbrvvvt08OBBDR8+vNrQjT+ETLCQJPfcGIIFAISX9evXa+rUqZo0aZIuu+wytW7dWvv27WvSGhwOh1JTU5Wbm+vZVllZqS1btjT4Pbt3766Kigp99tlnnm3Hjh3Trl271KNHD8+2jIwM3XPPPXrnnXc0a9YszZ8/3/NYy5YtNWXKFL3++uv685//rFdffbXB9dRHyAyFSOYEzogIaetWqaBAysy0uiIAQFPo3Lmz3nnnHY0fP142m02PPvpog4cfGuM3v/mNsrOz1alTJ3Xr1k3PP/+8jh8/Xq8ryubl5SkxMdFz32azqU+fPpowYYL+67/+S6+88ooSExP10EMP6dJLL9WECRMkSTNnztSYMWPUpUsXHT9+XKtXr1b37t0lSbNnz1a/fv3Us2dPlZWV6b333vM85i8hFSxatpSGDJHWrpWWLpWmT7e6IgBAU3j22Wf1y1/+UldddZVatGihBx98UE6ns8nrePDBB3Xo0CHdcccdioyM1F133aVRo0Ypsh4LLF199dXV7kdGRqqiokILFizQjBkzdMMNN6i8vFxXX3213n//fc+wTGVlpbKysnTgwAElJSVp9OjRmjdvniRzLY6HH35Y+/btU1xcnIYOHapFixb5/g+vwmY08eCQ0+mUw+FQcXGxkpKSfP7+8+ZJ998vXXuttGqVz98eAIJKaWmp8vPz1b59+wZfBhsN53K51L17d/3kJz/RH/7wB6vLqdPFvi/1/f0OqTkW0rl5FmvXSseOWVoKACDMFBQUaP78+frmm2+Ul5enadOmKT8/Xz//+c+tLq3JhFywaN9e6tNHqqyU3nvP6moAAOEkIiJCCxcu1IABAzR48GDl5eVp5cqVfp/XEEhCao6F28SJ0hdfmGeHTJlidTUAgHCRkZGh9evXW12GpULuiIV0bjjkww99u4QsAAC4uJAMFn36mKeanjkjrVhhdTUAYD2rF3FCcPDF9yQkg4XNdu6oxeLFlpYCAJZyn5J4msO3qAf39+T8FUa9EZJzLCRp0iTpL3+Rli2TKirMS6sDQLiJjIxUcnKy53oW8fHx9VqsCeHFMAydPn1aR44cUXJycr3W3ahNyP7cDh4spaSYp5yuWycNG2Z1RQBgjdatW0vSRS+uBUhScnKy5/vSUCEbLKKipPHjpYULzbNDCBYAwpXNZlNaWppatWqls2fPWl0OAlR0dHSjjlS4hWywkMx5Fu5gMW+eb6+sBwDBJjIy0ic/HMDFhOTkTbfrr5fi4swLkn3xhdXVAAAQ+kI6WMTHS6NHm33ODgEAwP9COlhI5047XbLEyioAAAgPIR8sxo2TIiOlL7+U9u61uhoAAEJbyAeLlBTJfYn7pUutrQUAgFAX8sFCYjgEAICmEhbBYsIE83bdOun7762tBQCAUBYWwSIzU7r8csnlMpf4BgAA/hEWwUJiOAQAgKYQdsHio4+kU6csLQUAgJAVNsGiVy+pQweprEz68EOrqwEAIDSFTbCw2c4dteC0UwAA/CNsgoV07uyQ//xHqqiwthYAAEJRWAWLq66SLrlE+uEHaf16q6sBACD0hFWwiIqSbrjB7DMcAgCA74VVsJDODYcsXSoZhrW1AAAQasIuWIwcKdnt5gXJtm+3uhoAAEJL2AWLhARpxAizz3AIAAC+5XWw+O6773TbbbcpJSVFcXFxuuyyy7Rp0yZ/1OY3VYdDAACA70R58+Tjx49r8ODBuvbaa7V8+XK1bNlSu3fvVvPmzf1Vn1+MH2+ua5GbKx08KKWnW10RAAChwatg8dRTTykjI0MLFizwbGvfvv1FX1NWVqaysjLPfafT6WWJvte6tTRwoLRxo/Tuu9I991hdEQAAocGroZB3331X/fv31y233KJWrVqpb9++mj9//kVfk52dLYfD4WkZGRmNKthXGA4BAMD3bIZR/5MuY2NjJUn333+/brnlFuXm5mrGjBl6+eWXNWXKlBpfU9MRi4yMDBUXFyspKamR5Tfcjh1Sjx5STIx09KiUmGhZKQAABDyn0ymHw1Hn77dXwSImJkb9+/fXhg0bPNumT5+u3Nxcffrppz4tzN8MQ+raVdq9W3r7bemWWywrBQCAgFff32+vhkLS0tLUo0ePatu6d++uwsLChlVpIZtNuvFGs89wCAAAvuFVsBg8eLB27dpVbds333yjzMxMnxbVVKpelOzsWWtrAQAgFHgVLO677z5t3LhRTz75pPbs2aM33nhDr776qrKysvxVn19ddZXUooV04oT0ySdWVwMAQPDzKlgMGDBAixcv1ptvvqlevXrpD3/4g/785z9r8uTJ/qrPryIjuSgZAAC+5NXkTV8IlMmbbkuWSJMmSZmZUn6+OfcCAABU55fJm6Ho+uul2FipoEDKy7O6GgAAglvYB4tmzcxwITEcAgBAY4V9sJBYhRMAAF8hWMicwGmzSZs3SwcOWF0NAADBi2AhKTVVGjTI7L/7rrW1AAAQzAgW/4fhEAAAGo9g8X/cy3uvXi0VF1tbCwAAwYpg8X+6dZO6dDGX9v7gA6urAQAgOBEsqmA4BACAxiFYVOEOFu+/z0XJAABoCIJFFVdeKbVsac6xyMmxuhoAAIIPwaKKyEhp/Hizz2mnAAB4j2BxHvfVTpcvt7YOAACCEcHiPMOHS9HR0p490u7dVlcDAEBwIVicJylJGjLE7HPUAgAA7xAsajBmjHn7/vvW1gEAQLAhWNRg7Fjzds0a6fRpS0sBACCoECxq0KOHlJEhlZWZ4QIAANQPwaIGNtu5oxYMhwAAUH8Ei1q451ksXy4ZhrW1AAAQLAgWtbjuOvO00717Oe0UAID6IljUIjFRuvpqs89wCAAA9UOwuIiqwyEAAKBuBIuLcAeLnByppMTaWgAACAYEi4vo3l3KzDRPO1292upqAAAIfASLi7DZGA4BAMAbBIs6VF3PgtNOAQC4OIJFHa67ToqJkfbtk3btsroaAAACG8GiDs2anTvtlOEQAAAujmBRD+7hEIIFAAAXR7Coh6qnnZ46ZW0tAAAEMoJFPXTtKrVvL5WXc9opAAAXQ7CoB047BQCgfggW9eQOFpx2CgBA7QgW9XTttZLdLhUUSDt3Wl0NAACBiWBRT82aSddcY/YZDgEAoGYECy9UHQ4BAAAXIlh4wb2exSefcNopAAA1IVh4oXNnqUMH87TTVausrgYAgMBDsPCCzVb9omQAAKA6goWXqq5nwWmnAABUR7Dw0rBh5mmnhYXSjh1WVwMAQGDxKljMmTNHNputWuvWrZu/agtI8fHmmhYSwyEAAJzP6yMWPXv2VFFRkaetW7fOH3UFNJb3BgCgZlFevyAqSq1bt/ZHLUFj1Cjzdv166cwZKS7O2noAAAgUXh+x2L17t9LT09WhQwdNnjxZhYWFF31+WVmZnE5ntRbsunSR0tOlsjJpwwarqwEAIHB4FSwGDhyohQsX6oMPPtBLL72k/Px8DR06VCdPnqz1NdnZ2XI4HJ6WkZHR6KKtZrNJw4eb/Y8/trYWAAACic0wGn7S5IkTJ5SZmalnn31Wv/rVr2p8TllZmcrKyjz3nU6nMjIyVFxcrKSkpIZ+tOX+53+kqVOlgQOljRutrgYAAP9yOp1yOBx1/n57PceiquTkZHXp0kV79uyp9Tl2u112u70xHxOQrrvOvM3NlYqLJYfD2noAAAgEjVrH4tSpU/r222+Vlpbmq3qCRkaGucS3yyXl5FhdDQAAgcGrYPHAAw8oJydH+/bt04YNGzRp0iRFRkbqZz/7mb/qC2jMswAAoDqvgsWBAwf0s5/9TF27dtVPfvITpaSkaOPGjWrZsqW/6gto7mDBBckAADA1avJmQ9R38kcwOHpUcmeqQ4ek1FRr6wEAwF/q+/vNtUIaoUUL6Uc/MvsctQAAgGDRaAyHAABwDsGikZjACQDAOQSLRho6VIqKkvLzzQYAQDgjWDRSQoK5+qbEUQsAAAgWPsA8CwAATAQLH3Av771qldS0J+8CABBYCBY+cOWVUlycdPiwtH271dUAAGAdgoUP2O3mJE6J4RAAQHgjWPgIp50CAECw8Bn3PIs1a6SKCktLAQDAMgQLH+nbV0pOlpxOafNmq6sBAMAaBAsfiYyUrr3W7DPPAgAQrggWPuQeDmGeBQAgXBEsfMg9gXP9eqm01NpaAACwAsHCh7p1k9LSzFCxYYPV1QAA0PQIFj5ks7G8NwAgvBEsfIx5FgCAcEaw8DH3EYvcXPPUUwAAwgnBwsfatpU6dZIqK6W1a62uBgCApkWw8AOGQwAA4Ypg4QdcNwQAEK4IFn7gXoEzL086csTaWgAAaEoECz9o2VLq08fsr15tbS0AADQlgoWfMM8CABCOCBZ+wjwLAEA4Ilj4ydVXSxER0t690nffWV0NAABNg2DhJ4mJ5+ZZcN0QAEC4IFj40VVXmbfr11tbBwAATYVg4UeDB5u3BAsAQLggWPiRO1hs3SqVlFhbCwAATYFg4UcZGdKll5rXDcnNtboaAAD8j2DhRzYbwyEAgPBCsPAzggUAIJwQLPzMHSw+/VRyuaytBQAAfyNY+Fnv3lJ8vHTihLRjh9XVAADgXwQLP4uOlgYONPsMhwAAQh3BogkwzwIAEC4IFk3AvQInS3sDAEIdwaIJDBpknnq6Z490+LDV1QAA4D8EiyaQnCz17Gn2OWoBAAhlBIsmwjwLAEA4IFg0EeZZAADCQaOCxdy5c2Wz2TRz5kwflRO63EcsNm+WSkutrQUAAH9pcLDIzc3VK6+8ot69e/uynpDVoYOUmiqVl0ubNlldDQAA/tGgYHHq1ClNnjxZ8+fPV/PmzX1dU0iy2RgOAQCEvgYFi6ysLI0bN04jRoyo87llZWVyOp3VWrhiAicAINRFefuCRYsWacuWLcrNza3X87Ozs/X44497XVgocgeLDRskwzCPYgAAEEq8OmKxf/9+zZgxQ//4xz8UGxtbr9c8/PDDKi4u9rT9+/c3qNBQcPnlkt0uHT0qffON1dUAAOB7XgWLzZs368iRI7r88ssVFRWlqKgo5eTk6LnnnlNUVJQqKysveI3dbldSUlK1Fq5iYqQBA8w+8ywAAKHIq2AxfPhw5eXladu2bZ7Wv39/TZ48Wdu2bVNkZKS/6gwZzLMAAIQyr+ZYJCYmqlevXtW2NWvWTCkpKRdsR80IFgCAUMbKm01s0CDzdudO6dgxa2sBAMDXvD4r5Hxr1qzxQRnho0ULqWtXadcu6dNPpRtusLoiAAB8hyMWFmA4BAAQqggWFiBYAABCFcHCAu6lvXNzzWuHAAAQKggWFujaVUpJMa9yunWr1dUAAOA7BAsLVL0gGcMhAIBQQrCwCMECABCKCBYWOf+CZAAAhAKChUX695eio6VDh6T8fKurAQDANwgWFomLk/r1M/sMhwAAQgXBwkLueRZc6RQAECoIFhZioSwAQKghWFjIHSy++ko6ccLSUgAA8AmChYVSU6UOHcyzQj7/3OpqAABoPIKFxQYONG8JFgCAUECwsJg7WHz2mbV1AADgCwQLi11xhXn7+ecslAUACH4EC4v17StFRUlHjkgFBVZXAwBA4xAsLBYbK/XpY/aZZwEACHYEiwDAPAsAQKggWAQAggUAIFQQLAKAewLnli3S2bPW1gIAQGMQLAJAly6SwyGdOWOuwgkAQLAiWASAiAhpwACzzwROAEAwI1gECOZZAABCAcEiQFRdKAsAgGBFsAgQ7iMWX38tOZ3W1gIAQEMRLAJEaqqUmWku671pk9XVAADQMASLAMJwCAAg2BEsAggTOAEAwY5gEUA4YgEACHYEiwDSr58UGSkdPCgdOGB1NQAAeI9gEUDi46XLLjP7HLUAAAQjgkWAcQ+HMM8CABCMCBYBhgmcAIBgRrAIMO4jFps2SZWV1tYCAIC3CBYBpnt3KSFBKikxV+EEACCYECwCTGSk1L+/2WcCJwAg2BAsAhDzLAAAwYpgEYDcwYIjFgCAYEOwCEDuCZx5eeZcCwAAggXBIgBdeqnZXC5p82arqwEAoP4IFgGK64YAAIIRwSJAMYETABCMvAoWL730knr37q2kpCQlJSVp0KBBWr58ub9qC2tM4AQABCOvgkWbNm00d+5cbd68WZs2bdJ1112nCRMmaPv27f6qL2z16yfZbFJhoXTokNXVAABQP14Fi/Hjx2vs2LHq3LmzunTpoj/+8Y9KSEjQxo0b/VVf2EpMlHr2NPsctQAABIsGz7GorKzUokWLVFJSokGDBtX6vLKyMjmdzmoN9cOVTgEAwcbrYJGXl6eEhATZ7Xbdc889Wrx4sXr06FHr87Ozs+VwODwtIyOjUQWHE+ZZAACCjc0wDMObF5SXl6uwsFDFxcX617/+pb/97W/KycmpNVyUlZWprKzMc9/pdCojI0PFxcVKSkpqXPUhbts2qW9fKSlJOn5ciuAcHgCARZxOpxwOR52/314Hi/ONGDFCHTt21CuvvOLTwiBVVEgOh3T6tHml0+7dra4IABCu6vv73eh/A7tcrmpHJOA7UVHm2SESwyEAgODgVbB4+OGHtXbtWu3bt095eXl6+OGHtWbNGk2ePNlf9YU9JnACAIJJlDdPPnLkiO644w4VFRXJ4XCod+/e+vDDD3X99df7q76wxwROAEAwafQcC28xx8I7BQVSu3bmsIjTKcXFWV0RACAcNdkcC/hX27ZSaqo5kXPbNqurAQDg4ggWAc5mOzccsmGDtbUAAFAXgkUQGDLEvP3kE2vrAACgLgSLIDB0qHm7bp3kcllbCwAAF0OwCAKXXy7Fx0vHjkk7dlhdDQAAtSNYBIGYGOnKK80+wyEAgEBGsAgSV19t3hIsAACBjGARJNzzLNaulZp25REAAOqPYBEkrrzSXCTrwAFz0SwAAAIRwSJIxMdL/fub/bVrra0FAIDaECyCiHs4hHkWAIBARbAIIu4JnByxAAAEKoJFEBk82Fzi+5tvpMOHra4GAIALESyCSPPm0mWXmf1166ytBQCAmhAsgkzV004BAAg0BIsgw0JZAIBARrAIMu4jFtu2ScXFlpYCAMAFCBZBJi1N6tTJXH1zwwarqwEAoDqCRRBingUAIFARLIIQC2UBAAIVwSIIuSdw5uZKZ85YWwsAAFURLIJQhw7mXIvycunzz62uBgCAcwgWQchm47RTAEBgIlgEKSZwAgACEcEiSLmPWGzYIFVUWFsLAABuBIsg1bOnee2QkhJp61arqwEAwESwCFIREebVTiXmWQAAAgfBIoi5h0OYZwEACBQEiyDmnsC5bp3kcllbCwAAEsEiqF1+uRQfLx07Ju3caXU1AAAQLIJaTIx05ZVmn+EQAEAgIFgEORbKAgAEEoJFkKu6UJZhWFsLAAAEiyB35ZVSVJR04IBUUGB1NQCAcEewCHLx8VL//mafeRYAAKsRLEKAeziEeRYAAKsRLEIAEzgBAIGCYBECBg82L6W+a5d0+LDV1QAAwhnBIgQ0by5ddpnZX7fO2loAAOGNYBEi3PMsVq+2tg4AQHgjWISIkSPN22XLWM8CAGAdgkWIuP5689TTwkJp2zarqwEAhCuvgkV2drYGDBigxMREtWrVShMnTtSuXbv8VRu8EBcnjR5t9hcvtrYWAED48ipY5OTkKCsrSxs3btSKFSt09uxZjRw5UiUlJf6qD16YONG8XbLEyioAAOHMZhgNH5H//vvv1apVK+Xk5Ohq92IKdXA6nXI4HCouLlZSUlJDPxo1+OEHqVUrqbJS2rNH6tjR6ooAAKGivr/fjZpjUVxcLEm65JJLan1OWVmZnE5ntQb/uOQSadgws89RCwCAFRocLFwul2bOnKnBgwerV69etT4vOztbDofD0zIyMhr6kagHhkMAAFZq8FDItGnTtHz5cq1bt05t2rSp9XllZWUqKyvz3Hc6ncrIyGAoxE/275fatjVX4jx0yBwaAQCgsfw6FHLvvffqvffe0+rVqy8aKiTJbrcrKSmpWoP/ZGRI/fqZa1ksW2Z1NQCAcONVsDAMQ/fee68WL16sVatWqX379v6qC43gHg7htFMAQFPzKlhkZWXp9ddf1xtvvKHExEQdOnRIhw4d0pkzZ/xVHxpg0iTzduVK6eRJa2sBAIQXr4LFSy+9pOLiYg0bNkxpaWme9tZbb/mrPjRAjx5Sp05SWZn04YdWVwMACCdeD4XU1KZOneqn8tAQNhtnhwAArMG1QkKUO1i8955UXm5pKQCAMEKwCFFXXimlpkrFxVJOjtXVAADCBcEiREVGSjfeaPYZDgEANBWCRQirOs/C5bKyEgBAuCBYhLDrrpMSEqSDB6VNm6yuBgAQDggWISw2Vho71uwzHAIAaAoEixDHaacAgKZEsAhxY8dK0dHSjh3Srl1WVwMACHUEixDncEjXXmv2OWoBAPA3gkUYYDgEANBUCBZhYMIE83bjRqmoyNpaAAChjWARBtLTpYEDzf7SpdbWAgAIbQSLMMFwCACgKRAswoQ7WKxaZV4/BAAAfyBYhIlu3cx29qz0/vtWVwMACFUEizDiPmrxzjuWlgEACGEEizByyy3m7ZIlUmGhpaUAAEIUwSKMXH65eWGyigrpmWesrgYAEIoIFmHmd78zb+fPl44csbYWAEDoIViEmeuukwYMkEpLpb/8xepqAAChhmARZmy2c0ctXniBU08BAL5FsAhDN94o9eghOZ3SX/9qdTUAgFBCsAhDERHSQw+Z/XnzpNOnra0HABA6CBZh6qc/ldq1k77/Xvr7362uBgAQKggWYSo6Wvrtb83+00+bK3ICANBYBIsw9otfSKmp5mJZb7xhdTUAgFBAsAhjcXHSffeZ/blzJZfL2noAAMGPYBHmpk2THA5p504uqQ4AaDyCRZhLSpJ+8xuzn50tGYa19QAAghvBApo+3RwW2bRJWrnS6moAAMGMYAG1bCnddZfZf/JJa2sBAAQ3ggUkSbNmmaegrlkjffqp1dUAAIIVwQKSpIwM6fbbzX52trW1AACCF8ECHv/v/5kXKVu2TMrLs7oaAEAwIljAo2tX6cc/Nvtz51pbCwAgOBEsUM3DD5u3b74p5eRYWwsAIPgQLFBN377SnXea61nccYdUXGx1RQCAYEKwwAWefVbq0MG8hoh78SwAAOqDYIELJCZKr78uRURIr70m/fOfVlcEAAgWBAvUaNAg6Xe/M/t33y1995219QAAggPBArWaPVvq3186fty8xDpXPwUA1IVggVpFR5tDInFx0ooV0gsvWF0RACDQESxwUV27Ss88Y/YffFD6+mtr6wEABDavg8XatWs1fvx4paeny2azacmSJX4oC4Fk2jRp9GiptFS67TapvNzqigAAgcrrYFFSUqI+ffroxRdf9Ec9CEA2m/T3v0spKdLWrdKcOVZXBAAIVFHevmDMmDEaM2aMP2pBAEtLk159Vbr5Zumpp6SxY6UhQ6yuCgAQaPw+x6KsrExOp7NaQ3C66SZp6lTz7JDbb5f4nxIAcD6/B4vs7Gw5HA5Py8jI8PdHwo/+8hepXTtp3z5pxgyrqwEABBq/B4uHH35YxcXFnrZ//35/fyT8KCnJXI0zIkJauNAMFxUVVlcFAAgUfg8WdrtdSUlJ1RqC25Ah5vVEJOm556Qbb2RYBABgYh0LNMiMGdK//mUunrV8uTR4sFRQYHVVAACreR0sTp06pW3btmnbtm2SpPz8fG3btk2FhYW+rg0B7uabpbVrpdatpa++kgYOlD77zOqqAABW8jpYbNq0SX379lXfvn0lSffff7/69u2r2bNn+7w4BL7+/aXPP5f69JEOH5aGDZPeftvqqgAAVvF6HYthw4bJMAx/1IIglZEhrVsn/exn0nvvSbfeKn3zjfT735uLawEAwgdzLOATCQnSkiXSffeZ9x99VLrjDqmszNKyAABNjGABn4mMNM8Weflls//669Lw4dL27VZXBgBoKgQL+Nzdd0sffCA5HNL69VKvXuYpqRs2WF0ZAMDfCBbwixEjzEmdN99szrNYtsw8JXXoUOk//5GYpgMAoYlgAb/p0sVc62LHDunOO6WYGHOS5w03SL17myt4nj1rdZUAAF8iWMDvunaV5s+X8vOl3/5WSkw017244w6pUyfz+iPHj1tdJQDAFwgWaDLp6dKf/iQVFkrZ2VJqqtmfOVNq1UoaNUp65RVzPQwAQHCyGU28KIXT6ZTD4VBxcTHXDQlzpaXS//yP9OKLUl7eue02m3k9kptukiZNkjIzrasRAGCq7+83wQIB4ZtvpMWLpX//W8rNrf5Y//5myOjXzzyy0aqV1LKlFB1tTa0AEI4IFghahYXmYlv//rf0ySe1n0FyySVmyEhNPRc42raVunUzW4cOUpTXa8sCAGpCsEBIOHxYevdd83TVffukI0ek77+XXK66XxsdbU4OdQcNd2vfXkpKkux2v5cPACGDYIGQVVkp/fCDGTLc7fBhs+XnSzt3Srt2SadPX/x9YmLMM1SSkszbqv34eKm83FySvKzMnA/i7rvb2bNSWprUrt2FLSPDfH8ACBUEC4Q1l0s6cMAMGee3oiL/f77NZp4F066dOfm0amvb1rxt1sz/dQCArxAsgFpUVEinTklOp3Ty5Lnbqv3Tp80jDrGx5pCJ3X5hPyJCOnjQHKI5v505U3cdKSnnwkZGhnTppWYYufTSc/2EBL/uCgCot/r+fjO1DWEnKkpKTjabPxiGOQ9k3z5zaKagwGyFhef6Tqd07JjZtmyp/b2Sks4FjebNzeGXs2fNYRr3bdW+yyXFxZlDOXW1Zs3MVlM/Pt681ktSknlBOQCoL4IF4GM227mzVK64oubnFBefCxkFBeawzcGD0nffnWvuoypOp7ksulUcjnNBLDnZDDjuflJS3QEmNtYMJ1FR5q27Vb0vmX+v+8jR+UeQ3K2kxDyaVNvt6dNmfe3amZN027ev3k9MtGgn+pFhmPOL3EN9xcXmJOWePc2/mWCIpsZQCBCgTp48FzIOHjR/MKKjzSGamm6jo83hmdLScz+yNbWafpTP75eUmBNUQ01KSvWg0b69eVpy+/bmkFQgninkcp07InXwYM3zhk6cqPm1cXFS9+5myKjaMjPN7wrgDeZYAGiUsjIzzJw4YV7L5cSJ6u34cfMoQ23Bxd1KS80zeSorzfkt7v75pwxHRl54ds75raYhm6q3cXHS0aPmEJR7KMrdfvjh4n+ve8KtO2xceum5z0xIqH7r7sfGmj/47rOGSkur98vKzPk2JSXnjrq4j8xUPUJz6pT5fPfQVtVWUVH3/1YREWbd3bqZ+27HDrPVFg7j4sy/NS2t9ta6tRm0bDbz/Wu7DfSAcvastH+/+R3Yu9ds+fnmfj971ty/7nb+/WbNgiuE+hvBAkBAM4xzIcMwzv2I+YvTeWHYqNpKSvz32b4SH29e1K979+prs3TubIacqiorpW+/lbZvr9527TIDi6/ExtY8TFa1Hx9/YRipKagYRt3N5TKbO5ye33cHCXeIKCw0H/elqiE0I6N64K0adqu2yEjz76ztNirKDKvJyebwYyCerk6wAIB6ck+4rRo0iorMsHHq1LnhofP7paUXnjVU9dbdT0gwm/vIi7tf9TYuzvwxuViLjW18+KqoMANWUdHF27FjPtm1AcFuN482uFv79mbwiYo616Kjq9+PijKP2LmPdDR1CI2Prz63yT3XyW6vX0h74gnzNb5EsAAANJh72MrlOnek4Pxbl8v8kT1/qKzq/ePHzeGgml57ft9mq7u5/4Vf9V/7Ve9HRppHE6oGidatfTdkYxjnhtv27jVD2PlDgDVNKq7tKIv7tqLi3NCYLxQVmX+3L3G6KQCgwdz/aq9LixbhdQVim828CGLLlrWf9dUYlZXmsN35c5rcrbz84uHM3bdyDRyCBQAAASIy0hymad7c6koaLsDn8wIAgGBCsAAAAD5DsAAAAD5DsAAAAD5DsAAAAD5DsAAAAD5DsAAAAD5DsAAAAD5DsAAAAD5DsAAAAD5DsAAAAD5DsAAAAD5DsAAAAD7T5Fc3NQxDknlddwAAEBzcv9vu3/HaNHmwOHnypCQpIyOjqT8aAAA00smTJ+VwOGp93GbUFT18zOVy6eDBg0pMTJTNZvPZ+zqdTmVkZGj//v1KSkry2fuiZuzvpsX+blrs76bF/m5aDd3fhmHo5MmTSk9PV0RE7TMpmvyIRUREhNq0aeO3909KSuKL2YTY302L/d202N9Ni/3dtBqyvy92pMKNyZsAAMBnCBYAAMBnQiZY2O12PfbYY7Lb7VaXEhbY302L/d202N9Ni/3dtPy9v5t88iYAAAhdIXPEAgAAWI9gAQAAfIZgAQAAfIZgAQAAfIZgAQAAfCZkgsWLL76odu3aKTY2VgMHDtTnn39udUkhYe3atRo/frzS09Nls9m0ZMmSao8bhqHZs2crLS1NcXFxGjFihHbv3m1NsUEuOztbAwYMUGJiolq1aqWJEydq165d1Z5TWlqqrKwspaSkKCEhQTfffLMOHz5sUcXB76WXXlLv3r09KxAOGjRIy5cv9zzO/vafuXPnymazaebMmZ5t7G/fmjNnjmw2W7XWrVs3z+P+2t8hESzeeust3X///Xrssce0ZcsW9enTR6NGjdKRI0esLi3olZSUqE+fPnrxxRdrfPxPf/qTnnvuOb388sv67LPP1KxZM40aNUqlpaVNXGnwy8nJUVZWljZu3KgVK1bo7NmzGjlypEpKSjzPue+++7Rs2TL985//VE5Ojg4ePKibbrrJwqqDW5s2bTR37lxt3rxZmzZt0nXXXacJEyZo+/btktjf/pKbm6tXXnlFvXv3rrad/e17PXv2VFFRkaetW7fO85jf9rcRAq644gojKyvLc7+ystJIT083srOzLawq9EgyFi9e7LnvcrmM1q1bG08//bRn24kTJwy73W68+eabFlQYWo4cOWJIMnJycgzDMPdtdHS08c9//tPznB07dhiSjE8//dSqMkNO8+bNjb/97W/sbz85efKk0blzZ2PFihXGNddcY8yYMcMwDL7f/vDYY48Zffr0qfExf+7voD9iUV5ers2bN2vEiBGebRERERoxYoQ+/fRTCysLffn5+Tp06FC1fe9wODRw4ED2vQ8UFxdLki655BJJ0ubNm3X27Nlq+7tbt25q27Yt+9sHKisrtWjRIpWUlGjQoEHsbz/JysrSuHHjqu1Xie+3v+zevVvp6enq0KGDJk+erMLCQkn+3d9NfnVTXzt69KgqKyuVmppabXtqaqp27txpUVXh4dChQ5JU4753P4aGcblcmjlzpgYPHqxevXpJMvd3TEyMkpOTqz2X/d04eXl5GjRokEpLS5WQkKDFixerR48e2rZtG/vbxxYtWqQtW7YoNzf3gsf4fvvewIEDtXDhQnXt2lVFRUV6/PHHNXToUH311Vd+3d9BHyyAUJSVlaWvvvqq2ngo/KNr167atm2biouL9a9//UtTpkxRTk6O1WWFnP3792vGjBlasWKFYmNjrS4nLIwZM8bT7927twYOHKjMzEy9/fbbiouL89vnBv1QSIsWLRQZGXnBTNbDhw+rdevWFlUVHtz7l33vW/fee6/ee+89rV69Wm3atPFsb926tcrLy3XixIlqz2d/N05MTIw6deqkfv36KTs7W3369NFf/vIX9rePbd68WUeOHNHll1+uqKgoRUVFKScnR88995yioqKUmprK/vaz5ORkdenSRXv27PHr9zvog0VMTIz69eunjz/+2LPN5XLp448/1qBBgyysLPS1b99erVu3rrbvnU6nPvvsM/Z9AxiGoXvvvVeLFy/WqlWr1L59+2qP9+vXT9HR0dX2965du1RYWMj+9iGXy6WysjL2t48NHz5ceXl52rZtm6f1799fkydP9vTZ3/516tQpffvtt0pLS/Pv97tRUz8DxKJFiwy73W4sXLjQ+Prrr4277rrLSE5ONg4dOmR1aUHv5MmTxtatW42tW7cakoxnn33W2Lp1q1FQUGAYhmHMnTvXSE5ONpYuXWp8+eWXxoQJE4z27dsbZ86csbjy4DNt2jTD4XAYa9asMYqKijzt9OnTnufcc889Rtu2bY1Vq1YZmzZtMgYNGmQMGjTIwqqD20MPPWTk5OQY+fn5xpdffmk89NBDhs1mMz766CPDMNjf/lb1rBDDYH/72qxZs4w1a9YY+fn5xvr1640RI0YYLVq0MI4cOWIYhv/2d0gEC8MwjOeff95o27atERMTY1xxxRXGxo0brS4pJKxevdqQdEGbMmWKYRjmKaePPvqokZqaatjtdmP48OHGrl27rC06SNW0nyUZCxYs8DznzJkzxq9//WujefPmRnx8vDFp0iSjqKjIuqKD3C9/+UsjMzPTiImJMVq2bGkMHz7cEyoMg/3tb+cHC/a3b916661GWlqaERMTY1x66aXGrbfeauzZs8fzuL/2t80wDKNxxzwAAABMQT/HAgAABA6CBQAA8BmCBQAA8BmCBQAA8BmCBQAA8BmCBQAA8BmCBQAA8BmCBQAA8BmCBQAA8BmCBQAA8BmCBQAA8Jn/DyHY2dXgkH8dAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Take a look at the training curves of your model\n",
        "\n",
        "acc = history.history['accuracy']\n",
        "loss = history.history['loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'b', label='Training accuracy')\n",
        "plt.title('Training accuracy')\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'b', label='Training Loss')\n",
        "plt.title('Training loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "9QRG73l6qE-c",
        "tags": [],
        "outputId": "f820b162-f578-45be-c474-6c5ffecffb20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_967c9809-43b5-4fb9-b5f7-cd17bb2643cf\", \"history.pkl\", 942)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "def download_history():\n",
        "  import pickle\n",
        "  from google.colab import files\n",
        "\n",
        "  with open('history.pkl', 'wb') as f:\n",
        "    pickle.dump(history.history, f)\n",
        "\n",
        "  files.download('history.pkl')\n",
        "\n",
        "download_history()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdsMszk9zBs_"
      },
      "source": [
        "## See your model in action\n",
        "\n",
        "Having navigated through the various stages of model development, it's an exciting moment to witness the culmination of efforts. Running the cell below unveils the model's capability to generate the next 100 words based on a given seed text.\n",
        "\n",
        "Post-submission, I'm intrigued to explore further experimentation, adjusting training epochs to gauge their impact on text coherency. Additionally, varying the seed text promises diverse and captivating outcomes, offering a glimpse into the model's adaptability to different linguistic contexts.\n",
        "\n",
        "This marks a dynamic phase in the project, as the neural network transforms from a trained model to a creative text generator, showcasing the intricacies of language modeled after Shakespeare's sonnets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "6Vc6PHgxa6Hm",
        "tags": [],
        "outputId": "20f3604f-5246-48fc-f47c-5c2be7b2094c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help me Obi Wan Kenobi, you're my only hope my best acquainted beside behind you to is't in thee 'fair kind and thee of thee alas i have spent thee more in of thee receiving i in thee i am of thee 'fair in doubt wooing on that night doth latch hue thee is't not compared with me that i am not so wooing me alone of thee i feed on thee i derive sought sought is't not thee again and i thee prove thee more in in me more receiving in you can be the art of more of you of me untrue arising is't not disdains the\n"
          ]
        }
      ],
      "source": [
        "seed_text = \"Help me Obi Wan Kenobi, you're my only hope\"\n",
        "next_words = 100\n",
        "\n",
        "for _ in range(next_words):\n",
        "\t# Convert the text into sequences\n",
        "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\t# Pad the sequences\n",
        "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\t# Get the probabilities of predicting a word\n",
        "\tpredicted = model.predict(token_list, verbose=0)\n",
        "\t# Choose the next word based on the maximum probability\n",
        "\tpredicted = np.argmax(predicted, axis=-1).item()\n",
        "\t# Get the actual word from the word index\n",
        "\toutput_word = tokenizer.index_word[predicted]\n",
        "\t# Append to the current text\n",
        "\tseed_text += \" \" + output_word\n",
        "\n",
        "print(seed_text)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "dlai_version": "1.2.0",
    "jupytext": {
      "main_language": "python"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}